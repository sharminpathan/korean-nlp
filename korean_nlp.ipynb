{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'deeplearn'.\n",
      "NOTE: Added action set 'sampling'.\n",
      "NOTE: Added action set 'textParse'.\n",
      "NOTE: Added action set 'decisionTree'.\n"
     ]
    }
   ],
   "source": [
    "conn = swat.CAS(os.environ['CASHOST'], os.environ['CASPORT'])\n",
    "conn.loadactionset('deeplearn')\n",
    "conn.loadactionset('sampling')\n",
    "conn.loadactionset('textParse')\n",
    "conn.loadactionset('decisionTree')\n",
    "\n",
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO\n",
    "    \n",
    "import swat.cas.datamsghandlers as dmh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlpy\n",
    "from dlpy import Sequential\n",
    "from dlpy import *\n",
    "from dlpy.model import Model, Optimizer, AdamSolver\n",
    "from dlpy.model import TextParms\n",
    "from dlpy.blocks import Bidirectional\n",
    "from dlpy.applications import TextClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table ARTICLE_TRAIN in caslib CASUSER(shpath).\n",
      "NOTE: The table ARTICLE_TRAIN has been created in caslib CASUSER(shpath) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><caption>Selected Rows from Table ARTICLE_TRAIN</caption>\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"docID\">docID</th>\n",
       "      <th title=\"title\">title</th>\n",
       "      <th title=\"content\">content</th>\n",
       "      <th title=\"Category\">Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>51e91238eda5cbf2</td>\n",
       "      <td>쿠데타 관련 장성 47명 강퇴 터키, 99명 장군진급 잔치</td>\n",
       "      <td>레제프 타이이프 에르도안 대통령왼쪽이 군사평의회의 대규모 진급 결정을 승인하는 서명...</td>\n",
       "      <td>C5.중동아프리카</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>394c7a2574ddaceb</td>\n",
       "      <td>美히스패닉계 2명, 살인혐의로 23년 복역 후 무죄 석방</td>\n",
       "      <td>미국의 히스패닉계 남성 2명이 살인혐의로 수감된 지 23년 만에 무죄 석방됐다. 시...</td>\n",
       "      <td>C5.중동아프리카</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>3ee3c09e78bf8c03</td>\n",
       "      <td>터키, 3개월간 에르도안이 황제…사회각계서 6만명 숙청</td>\n",
       "      <td>에르도안 지지자들 [AFP=연합뉴스] 국가비상사태 선포…2002년 남동부서 종료된 ...</td>\n",
       "      <td>C5.중동아프리카</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>8826aa7529070c78</td>\n",
       "      <td>시리아 정부군에 투항하는 알레포 반군</td>\n",
       "      <td>【베이루트=AP/뉴시스】시리아 정부군이 북부 알레포에서 반군 포위작전을 3주간 펼치...</td>\n",
       "      <td>C5.중동아프리카</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>1b6d0ec5ce31bc9b</td>\n",
       "      <td>아프로 머리 소중하다 남아공 여학생들 차별 반대시위</td>\n",
       "      <td>남아공 프리토리아 여고 앞에서 생머리 퍼머를 강요하는 학칙에 항의해 시위를 벌이고 ...</td>\n",
       "      <td>C5.중동아프리카</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Selected Rows from Table ARTICLE_TRAIN\n",
       "\n",
       "                 docID                             title  \\\n",
       "3995  51e91238eda5cbf2  쿠데타 관련 장성 47명 강퇴 터키, 99명 장군진급 잔치   \n",
       "3996  394c7a2574ddaceb   美히스패닉계 2명, 살인혐의로 23년 복역 후 무죄 석방   \n",
       "3997  3ee3c09e78bf8c03    터키, 3개월간 에르도안이 황제…사회각계서 6만명 숙청   \n",
       "3998  8826aa7529070c78              시리아 정부군에 투항하는 알레포 반군   \n",
       "3999  1b6d0ec5ce31bc9b      아프로 머리 소중하다 남아공 여학생들 차별 반대시위   \n",
       "\n",
       "                                                content   Category  \n",
       "3995  레제프 타이이프 에르도안 대통령왼쪽이 군사평의회의 대규모 진급 결정을 승인하는 서명...  C5.중동아프리카  \n",
       "3996  미국의 히스패닉계 남성 2명이 살인혐의로 수감된 지 23년 만에 무죄 석방됐다. 시...  C5.중동아프리카  \n",
       "3997  에르도안 지지자들 [AFP=연합뉴스] 국가비상사태 선포…2002년 남동부서 종료된 ...  C5.중동아프리카  \n",
       "3998  【베이루트=AP/뉴시스】시리아 정부군이 북부 알레포에서 반군 포위작전을 3주간 펼치...  C5.중동아프리카  \n",
       "3999  남아공 프리토리아 여고 앞에서 생머리 퍼머를 강요하는 학칙에 항의해 시위를 벌이고 ...  C5.중동아프리카  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = 'C:/Users/shpath/Documents/korean/'\n",
    "article_train = conn.upload(path + 'news_training.sas7bdat', casout=dict(name='article_train', replace=True))\n",
    "train = conn.CASTable(name='article_train', casout='train')\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=train['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shpath\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table W2V_300 in caslib CASUSER(shpath).\n",
      "NOTE: The table W2V_300 has been created in caslib CASUSER(shpath) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CASTable('W2V_300', caslib='CASUSER(shpath)')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "embeddings = pd.DataFrame.from_csv(path + 'w2v_300.txt',sep='\\t')\n",
    "conn.upload_frame(embeddings, casout=dict(name='w2v_300', replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Output layer added.\n",
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "model = TextClassification(conn, neurons=128, n_blocks=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using SEED=464828720 for sampling.\n",
      "NOTE: Training from scratch.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3196.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost      35.49 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01           1.658     0.7995            1.611           0.7925     3.89\n",
      "NOTE:  1          0.01           1.649     0.8014            1.605           0.7775     4.60\n",
      "NOTE:  2          0.01           1.636     0.7947            1.599             0.75     4.03\n",
      "NOTE:  3          0.01           1.617     0.7673            1.598           0.7725     4.26\n",
      "NOTE:  4          0.01           1.622     0.7837            1.592           0.6825     3.93\n",
      "NOTE:  5          0.01           1.615     0.7654            1.584            0.665     4.01\n",
      "NOTE:  6          0.01           1.611     0.7639            1.578            0.635     3.91\n",
      "NOTE:  7          0.01           1.615     0.7688            1.578            0.675     3.60\n",
      "NOTE:  8          0.01            1.62     0.7649            1.568           0.6175     4.17\n",
      "NOTE:  9          0.01           1.596     0.7519            1.571           0.7475     3.79\n",
      "NOTE:  10         0.01           1.588     0.7341            1.559           0.5975     4.12\n",
      "NOTE:  11         0.01           1.596     0.7481            1.554           0.6025     4.01\n",
      "NOTE:  12         0.01            1.57     0.7087            1.548           0.6275     4.05\n",
      "NOTE:  13         0.01           1.568     0.7163            1.542           0.5975     4.00\n",
      "NOTE:  14         0.01           1.571     0.7188            1.536           0.5625     4.10\n",
      "NOTE:  15         0.01           1.558     0.6942             1.53           0.5575     4.20\n",
      "NOTE:  16         0.01           1.541     0.6947            1.526             0.57     4.09\n",
      "NOTE:  17         0.01           1.554     0.6937             1.52            0.585     4.14\n",
      "NOTE:  18         0.01           1.537     0.6822            1.513           0.5575     4.20\n",
      "NOTE:  19         0.01           1.535     0.6865            1.505             0.56     4.14\n",
      "NOTE:  20         0.01           1.525     0.6587              1.5           0.5275     4.26\n",
      "NOTE:  21         0.01           1.526     0.6731            1.492           0.5225     4.23\n",
      "NOTE:  22         0.01           1.512     0.6505             1.49           0.5975     4.10\n",
      "NOTE:  23         0.01           1.514     0.6553            1.479             0.53     3.95\n",
      "NOTE:  24         0.01           1.506     0.6442            1.472           0.5425     4.08\n",
      "NOTE:  25         0.01            1.48     0.6043            1.463           0.5225     4.02\n",
      "NOTE:  26         0.01           1.478     0.6101            1.457            0.485     4.14\n",
      "NOTE:  27         0.01           1.477     0.6226            1.451            0.525     4.28\n",
      "NOTE:  28         0.01           1.478     0.6202            1.442           0.4925     3.93\n",
      "NOTE:  29         0.01           1.464     0.6062            1.435           0.5075     3.95\n",
      "NOTE:  30         0.01           1.451     0.5798            1.426            0.505     3.84\n",
      "NOTE:  31         0.01           1.433     0.5822            1.418            0.505     3.79\n",
      "NOTE:  32         0.01           1.434     0.5808            1.412           0.5025     4.05\n",
      "NOTE:  33         0.01            1.42     0.5635            1.402           0.5475     4.02\n",
      "NOTE:  34         0.01           1.421     0.5736            1.394           0.5425     4.28\n",
      "NOTE:  35         0.01           1.411     0.5687            1.385              0.5     3.92\n",
      "NOTE:  36         0.01           1.402     0.5567            1.377           0.5175     4.08\n",
      "NOTE:  37         0.01           1.381     0.5447            1.369           0.5075     3.84\n",
      "NOTE:  38         0.01           1.391     0.5404             1.36           0.5075     4.12\n",
      "NOTE:  39         0.01           1.377     0.5365            1.351           0.4975     3.95\n",
      "NOTE:  40         0.01           1.359     0.5308            1.342              0.5     3.81\n",
      "NOTE:  41         0.01           1.354     0.5231            1.337           0.5075     4.12\n",
      "NOTE:  42         0.01           1.347     0.5163            1.327             0.51     3.96\n",
      "NOTE:  43         0.01            1.34     0.5125            1.319            0.515     4.30\n",
      "NOTE:  44         0.01            1.34     0.5327             1.31              0.5     3.84\n",
      "NOTE:  45         0.01           1.335      0.513            1.302           0.5025     4.22\n",
      "NOTE:  46         0.01           1.314     0.5024            1.294              0.5     4.25\n",
      "NOTE:  47         0.01           1.319      0.513            1.288           0.4925     3.92\n",
      "NOTE:  48         0.01           1.296     0.5087            1.279            0.485     4.10\n",
      "NOTE:  49         0.01           1.297     0.4976            1.272             0.49     3.96\n",
      "NOTE:  50         0.01           1.276     0.4962            1.264              0.5     3.86\n",
      "NOTE:  51         0.01           1.287     0.5106            1.258           0.4925     3.86\n",
      "NOTE:  52         0.01           1.255     0.4755             1.25             0.52     4.03\n",
      "NOTE:  53         0.01           1.256     0.4731            1.241            0.495     4.08\n",
      "NOTE:  54         0.01           1.264     0.4918            1.234             0.48     4.04\n",
      "NOTE:  55         0.01           1.242     0.4899            1.232             0.48     3.76\n",
      "NOTE:  56         0.01           1.244     0.4875            1.221             0.47     3.85\n",
      "NOTE:  57         0.01           1.239     0.4894            1.213            0.465     3.93\n",
      "NOTE:  58         0.01           1.224     0.4755            1.207           0.4775     4.02\n",
      "NOTE:  59         0.01           1.222     0.4663            1.202             0.47     3.74\n",
      "NOTE:  60         0.01           1.198     0.4567            1.195           0.4825     4.14\n",
      "NOTE:  61         0.01           1.192     0.4668            1.187             0.47     4.21\n",
      "NOTE:  62         0.01           1.196     0.4553            1.182             0.46     4.23\n",
      "NOTE:  63         0.01           1.207      0.462            1.176           0.4575     3.88\n",
      "NOTE:  64         0.01            1.18     0.4615            1.169             0.46     4.00\n",
      "NOTE:  65         0.01           1.182     0.4462            1.162           0.4625     3.98\n",
      "NOTE:  66         0.01           1.169     0.4519            1.156            0.465     3.92\n",
      "NOTE:  67         0.01           1.164     0.4423            1.153           0.4525     4.25\n",
      "NOTE:  68         0.01           1.158       0.45            1.144            0.445     4.18\n",
      "NOTE:  69         0.01           1.174     0.4462            1.141            0.445     3.81\n",
      "NOTE:  70         0.01           1.138     0.4327            1.134             0.44     3.81\n",
      "NOTE:  71         0.01           1.146     0.4437             1.13             0.44     3.85\n",
      "NOTE:  72         0.01           1.163     0.4553            1.125           0.4325     3.95\n",
      "NOTE:  73         0.01           1.107     0.4197            1.118           0.4425     3.84\n",
      "NOTE:  74         0.01           1.115     0.4163            1.111            0.435     3.91\n",
      "NOTE:  75         0.01           1.115     0.4202            1.107           0.4325     4.16\n",
      "NOTE:  76         0.01           1.095     0.4207            1.102           0.4225     3.84\n",
      "NOTE:  77         0.01           1.129     0.4375            1.097             0.43     4.34\n",
      "NOTE:  78         0.01            1.14      0.438            1.094            0.415     3.98\n",
      "NOTE:  79         0.01           1.117     0.4365            1.088           0.4075     4.02\n",
      "NOTE:  80         0.01           1.104     0.4178            1.082             0.42     4.01\n",
      "NOTE:  81         0.01             1.1     0.4072            1.081            0.405     3.97\n",
      "NOTE:  82         0.01           1.092      0.412            1.072            0.425     4.25\n",
      "NOTE:  83         0.01           1.094     0.4188            1.068            0.425     4.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  84         0.01           1.073     0.3947            1.062             0.42     3.89\n",
      "NOTE:  85         0.01           1.054     0.3837            1.058             0.42     4.06\n",
      "NOTE:  86         0.01           1.055     0.4029            1.054           0.4175     3.71\n",
      "NOTE:  87         0.01           1.064      0.412             1.05            0.415     4.14\n",
      "NOTE:  88         0.01           1.052      0.401            1.046           0.3925     3.74\n",
      "NOTE:  89         0.01           1.041     0.4067            1.044             0.39     4.04\n",
      "NOTE:  90         0.01            1.07     0.4115            1.036           0.4125     4.21\n",
      "NOTE:  91         0.01           1.039     0.3986            1.034           0.4075     4.25\n",
      "NOTE:  92         0.01           1.082     0.4111            1.029              0.4     3.95\n",
      "NOTE:  93         0.01           1.028     0.3995            1.029            0.405     4.20\n",
      "NOTE:  94         0.01           1.041     0.4048            1.021              0.4     3.69\n",
      "NOTE:  95         0.01           1.023      0.387            1.017              0.4     3.96\n",
      "NOTE:  96         0.01           1.049     0.4077            1.012              0.4     4.18\n",
      "NOTE:  97         0.01           1.051     0.3952            1.014            0.375     3.73\n",
      "NOTE:  98         0.01           1.001     0.3683            1.006            0.385     4.03\n",
      "NOTE:  99         0.01           1.012     0.3841            1.003              0.4     4.25\n",
      "NOTE:  100        0.01          0.9947     0.3721           0.9985             0.39     3.70\n",
      "NOTE:  101        0.01           1.033     0.4034           0.9951           0.3875     3.94\n",
      "NOTE:  102        0.01           1.001     0.3784           0.9931           0.3675     4.33\n",
      "NOTE:  103        0.01          0.9965      0.375           0.9892           0.3675     3.86\n",
      "NOTE:  104        0.01           1.008     0.3712           0.9846             0.38     3.97\n",
      "NOTE:  105        0.01           0.985     0.3601           0.9815           0.3975     3.95\n",
      "NOTE:  106        0.01               1     0.3788           0.9798           0.3625     4.03\n",
      "NOTE:  107        0.01           1.029     0.3812           0.9746           0.3725     3.86\n",
      "NOTE:  108        0.01          0.9743     0.3529           0.9718             0.37     3.89\n",
      "NOTE:  109        0.01          0.9818     0.3721           0.9699            0.365     4.04\n",
      "NOTE:  110        0.01          0.9799     0.3726           0.9647           0.3625     3.95\n",
      "NOTE:  111        0.01           1.014     0.3817           0.9665             0.36     3.96\n",
      "NOTE:  112        0.01          0.9649     0.3476           0.9589           0.3625     4.02\n",
      "NOTE:  113        0.01          0.9839      0.376           0.9571           0.3625     4.10\n",
      "NOTE:  114        0.01          0.9717     0.3596           0.9564            0.375     3.94\n",
      "NOTE:  115        0.01          0.9864      0.376            0.952            0.355     4.12\n",
      "NOTE:  116        0.01          0.9814     0.3654           0.9505           0.3525     4.21\n",
      "NOTE:  117        0.01          0.9611     0.3514           0.9447           0.3475     4.11\n",
      "NOTE:  118        0.01          0.9373     0.3534           0.9457            0.345     4.10\n",
      "NOTE:  119        0.01          0.9499     0.3736           0.9409            0.345     3.89\n",
      "NOTE:  120        0.01          0.9651     0.3587           0.9359           0.3475     3.81\n",
      "NOTE:  121        0.01           0.975     0.3779           0.9362             0.34     4.27\n",
      "NOTE:  122        0.01          0.9474     0.3582           0.9325           0.3475     3.95\n",
      "NOTE:  123        0.01          0.9624     0.3683           0.9285             0.36     3.94\n",
      "NOTE:  124        0.01          0.9854     0.3808           0.9275           0.3425     3.96\n",
      "NOTE:  125        0.01           0.934     0.3438           0.9258           0.3475     3.85\n",
      "NOTE:  126        0.01          0.9767      0.374           0.9232           0.3375     3.72\n",
      "NOTE:  127        0.01          0.9471     0.3558           0.9218             0.34     3.98\n",
      "NOTE:  128        0.01          0.9369     0.3514           0.9187           0.3375     3.92\n",
      "NOTE:  129        0.01          0.9647     0.3639           0.9157            0.335     3.76\n",
      "NOTE:  130        0.01           0.927     0.3385           0.9132            0.335     4.23\n",
      "NOTE:  131        0.01          0.9262     0.3538           0.9097             0.33     3.88\n",
      "NOTE:  132        0.01          0.9043      0.349           0.9083            0.325     3.91\n",
      "NOTE:  133        0.01          0.9249     0.3524           0.9043            0.325     3.79\n",
      "NOTE:  134        0.01          0.9285     0.3481           0.9052             0.32     4.06\n",
      "NOTE:  135        0.01          0.9226     0.3423           0.9016             0.32     3.81\n",
      "NOTE:  136        0.01          0.8873     0.3245           0.8951             0.31     3.93\n",
      "NOTE:  137        0.01          0.9252     0.3476           0.8959           0.3025     4.12\n",
      "NOTE:  138        0.01          0.9265     0.3524           0.8904             0.32     4.02\n",
      "NOTE:  139        0.01          0.9153     0.3327             0.89           0.3175     3.79\n",
      "NOTE:  140        0.01          0.9086     0.3337            0.886           0.3025     3.76\n",
      "NOTE:  141        0.01          0.9144     0.3428           0.8865             0.32     3.89\n",
      "NOTE:  142        0.01          0.9024     0.3274           0.8848           0.3325     4.16\n",
      "NOTE:  143        0.01          0.8738     0.3288            0.881           0.3275     3.80\n",
      "NOTE:  144        0.01          0.8998     0.3365           0.8779            0.315     4.10\n",
      "NOTE:  145        0.01          0.8957      0.337           0.8768             0.31     3.86\n",
      "NOTE:  146        0.01          0.8833     0.3317           0.8757           0.3275     3.95\n",
      "NOTE:  147        0.01          0.9168     0.3404           0.8718            0.305     3.90\n",
      "NOTE:  148        0.01           0.874     0.3298           0.8686              0.3     4.11\n",
      "NOTE:  149        0.01          0.8729     0.3269           0.8666           0.3025     4.00\n",
      "NOTE:  150        0.01          0.8824     0.3202           0.8646           0.3125     3.98\n",
      "NOTE:  151        0.01          0.8923     0.3187           0.8631             0.31     3.89\n",
      "NOTE:  152        0.01          0.8671     0.3111            0.863              0.3     3.94\n",
      "NOTE:  153        0.01          0.9092     0.3274           0.8574              0.3     4.09\n",
      "NOTE:  154        0.01          0.8925     0.3351           0.8567              0.3     3.90\n",
      "NOTE:  155        0.01          0.8632      0.325           0.8547              0.3     3.85\n",
      "NOTE:  156        0.01          0.8837     0.3356           0.8546             0.31     3.98\n",
      "NOTE:  157        0.01          0.8621     0.3178           0.8516           0.2925     3.91\n",
      "NOTE:  158        0.01          0.8691     0.3091           0.8501           0.2925     3.90\n",
      "NOTE:  159        0.01          0.8508     0.3043           0.8477           0.2975     3.91\n",
      "NOTE:  160        0.01          0.8574     0.3149           0.8452           0.2975     4.07\n",
      "NOTE:  161        0.01          0.8579     0.3173           0.8449           0.3025     3.88\n",
      "NOTE:  162        0.01          0.8559     0.3245           0.8425           0.3125     4.01\n",
      "NOTE:  163        0.01          0.8555     0.3149           0.8379           0.3025     3.82\n",
      "NOTE:  164        0.01          0.8667     0.3125           0.8359            0.295     3.89\n",
      "NOTE:  165        0.01           0.845     0.3212           0.8354            0.305     4.03\n",
      "NOTE:  166        0.01          0.8455     0.3014           0.8324            0.295     3.99\n",
      "NOTE:  167        0.01          0.8593      0.326           0.8309              0.3     4.11\n",
      "NOTE:  168        0.01          0.8456     0.3077           0.8285              0.3     3.85\n",
      "NOTE:  169        0.01          0.8277     0.2986           0.8282            0.305     4.07\n",
      "NOTE:  170        0.01          0.8536     0.3159            0.824             0.29     3.63\n",
      "NOTE:  171        0.01          0.8283     0.3173           0.8251           0.3025     3.98\n",
      "NOTE:  172        0.01          0.8189     0.2971           0.8187             0.29     3.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  173        0.01          0.8262     0.3139           0.8236            0.295     3.89\n",
      "NOTE:  174        0.01          0.8381     0.3091           0.8162            0.295     3.90\n",
      "NOTE:  175        0.01          0.8124     0.2957           0.8171             0.29     3.99\n",
      "NOTE:  176        0.01          0.8282     0.3048           0.8174             0.29     4.25\n",
      "NOTE:  177        0.01          0.8346     0.3058            0.811           0.2925     4.19\n",
      "NOTE:  178        0.01            0.84     0.3115           0.8094             0.29     3.86\n",
      "NOTE:  179        0.01          0.8315     0.3024           0.8045           0.2825     3.76\n",
      "NOTE:  180        0.01          0.8337     0.3034           0.8053           0.2825     4.18\n",
      "NOTE:  181        0.01          0.8356     0.3154           0.8021             0.29     3.89\n",
      "NOTE:  182        0.01          0.8486     0.3029            0.803           0.2825     4.10\n",
      "NOTE:  183        0.01          0.8028     0.2933           0.7971             0.27     3.66\n",
      "NOTE:  184        0.01          0.8112      0.299           0.7977            0.285     3.97\n",
      "NOTE:  185        0.01          0.7977     0.2923            0.793           0.2775     4.25\n",
      "NOTE:  186        0.01          0.8213     0.3005           0.7916           0.2775     4.03\n",
      "NOTE:  187        0.01          0.8056     0.2981           0.7911           0.2775     3.82\n",
      "NOTE:  188        0.01          0.7758     0.2861           0.7875             0.28     4.11\n",
      "NOTE:  189        0.01          0.8072     0.2861           0.7868            0.265     4.09\n",
      "NOTE:  190        0.01          0.7883     0.2918           0.7881            0.275     3.70\n",
      "NOTE:  191        0.01          0.7975     0.2986            0.781           0.2625     4.16\n",
      "NOTE:  192        0.01          0.7937     0.2938           0.7832           0.2825     4.13\n",
      "NOTE:  193        0.01          0.7769     0.2889           0.7783           0.2625     4.09\n",
      "NOTE:  194        0.01          0.7907     0.2894           0.7759           0.2625     3.90\n",
      "NOTE:  195        0.01          0.7876     0.2837           0.7754             0.26     3.89\n",
      "NOTE:  196        0.01          0.8133      0.288            0.776           0.2725     4.23\n",
      "NOTE:  197        0.01          0.7746     0.2769           0.7682           0.2575     3.89\n",
      "NOTE:  198        0.01          0.7974     0.2856            0.766            0.265     4.02\n",
      "NOTE:  199        0.01          0.7721     0.2837           0.7677           0.2675     4.15\n",
      "NOTE:  200        0.01          0.7741     0.2813           0.7652             0.26     4.03\n",
      "NOTE:  201        0.01          0.7651     0.2697           0.7625            0.265     4.22\n",
      "NOTE:  202        0.01          0.7736     0.2764           0.7626             0.27     3.90\n",
      "NOTE:  203        0.01          0.7632      0.288           0.7612           0.2575     3.94\n",
      "NOTE:  204        0.01          0.7775     0.2716           0.7611            0.275     4.07\n",
      "NOTE:  205        0.01          0.7696     0.2851           0.7597            0.275     4.16\n",
      "NOTE:  206        0.01          0.7703      0.287           0.7559           0.2675     3.83\n",
      "NOTE:  207        0.01          0.7973     0.2947           0.7498             0.27     3.99\n",
      "NOTE:  208        0.01           0.776     0.2851           0.7492           0.2725     4.18\n",
      "NOTE:  209        0.01          0.7328     0.2716           0.7493            0.265     4.00\n",
      "NOTE:  210        0.01          0.7397      0.275            0.747            0.265     4.01\n",
      "NOTE:  211        0.01          0.7689      0.276             0.75           0.2625     3.60\n",
      "NOTE:  212        0.01           0.766      0.288           0.7425           0.2575     4.12\n",
      "NOTE:  213        0.01           0.728     0.2611           0.7394             0.26     4.16\n",
      "NOTE:  214        0.01          0.7686     0.2923           0.7395           0.2675     3.90\n",
      "NOTE:  215        0.01            0.76     0.2687           0.7367             0.26     3.80\n",
      "NOTE:  216        0.01          0.7682     0.2846           0.7371           0.2675     3.99\n",
      "NOTE:  217        0.01          0.7442     0.2649           0.7384             0.26     3.99\n",
      "NOTE:  218        0.01          0.7681      0.275           0.7314           0.2575     4.01\n",
      "NOTE:  219        0.01          0.7436     0.2644           0.7276           0.2625     3.89\n",
      "NOTE:  220        0.01          0.7239     0.2635           0.7312           0.2675     3.89\n",
      "NOTE:  221        0.01          0.7411      0.274           0.7281           0.2575     4.07\n",
      "NOTE:  222        0.01            0.75     0.2736           0.7299            0.255     4.25\n",
      "NOTE:  223        0.01          0.7128     0.2611           0.7246           0.2575     3.99\n",
      "NOTE:  224        0.01          0.7418     0.2572           0.7236             0.26     3.85\n",
      "NOTE:  225        0.01          0.7077     0.2543           0.7199           0.2675     3.86\n",
      "NOTE:  226        0.01          0.7301     0.2548           0.7217           0.2575     3.95\n",
      "NOTE:  227        0.01          0.7104     0.2567            0.717             0.26     3.94\n",
      "NOTE:  228        0.01          0.7223     0.2562           0.7168             0.25     3.94\n",
      "NOTE:  229        0.01           0.711     0.2423           0.7171           0.2525     3.80\n",
      "NOTE:  230        0.01          0.7388     0.2673            0.718             0.26     3.93\n",
      "NOTE:  231        0.01          0.7339     0.2716           0.7259            0.265     4.05\n",
      "NOTE:  232        0.01          0.7512     0.2837           0.7143             0.26     4.07\n",
      "NOTE:  233        0.01          0.7353     0.2639           0.7118            0.245     3.84\n",
      "NOTE:  234        0.01          0.7013     0.2462           0.7102           0.2625     3.97\n",
      "NOTE:  235        0.01          0.7288      0.251           0.7078             0.25     4.08\n",
      "NOTE:  236        0.01           0.723     0.2543           0.7043           0.2575     4.21\n",
      "NOTE:  237        0.01          0.7339     0.2587           0.7031            0.245     4.25\n",
      "NOTE:  238        0.01          0.7163     0.2663           0.7008           0.2575     3.96\n",
      "NOTE:  239        0.01          0.7558     0.2813           0.6974             0.25     3.80\n",
      "NOTE:  240        0.01          0.7167     0.2514            0.697            0.255     3.93\n",
      "NOTE:  241        0.01          0.7004     0.2529           0.6972             0.25     3.81\n",
      "NOTE:  242        0.01          0.7186     0.2663            0.696           0.2525     3.94\n",
      "NOTE:  243        0.01          0.6801     0.2582           0.6915           0.2375     3.85\n",
      "NOTE:  244        0.01          0.6887     0.2524           0.6999           0.2575     3.84\n",
      "NOTE:  245        0.01          0.6737     0.2447           0.6895           0.2475     4.11\n",
      "NOTE:  246        0.01          0.6863     0.2591           0.6907             0.25     4.00\n",
      "NOTE:  247        0.01          0.6866     0.2505           0.6924            0.245     3.94\n",
      "NOTE:  248        0.01          0.7077     0.2572           0.6869           0.2525     3.99\n",
      "NOTE:  249        0.01            0.69     0.2418           0.6847           0.2475     3.87\n",
      "NOTE:  250        0.01          0.6619     0.2327           0.6817           0.2425     3.94\n",
      "NOTE:  251        0.01          0.6886     0.2462           0.6806             0.25     4.16\n",
      "NOTE:  252        0.01          0.6967     0.2524           0.6845           0.2575     4.10\n",
      "NOTE:  253        0.01          0.6965     0.2481           0.6751            0.245     4.03\n",
      "NOTE:  254        0.01           0.703     0.2668           0.6776           0.2475     3.86\n",
      "NOTE:  255        0.01          0.6889     0.2538           0.6746           0.2525     4.07\n",
      "NOTE:  256        0.01          0.6841     0.2481           0.6791           0.2375     3.98\n",
      "NOTE:  257        0.01          0.6943     0.2611           0.6777            0.245     3.94\n",
      "NOTE:  258        0.01          0.6798     0.2495           0.6675             0.24     3.68\n",
      "NOTE:  259        0.01          0.6892     0.2457           0.6748           0.2425     3.91\n",
      "NOTE:  260        0.01          0.6807     0.2495           0.6725            0.255     3.92\n",
      "NOTE:  261        0.01          0.6557     0.2399           0.6741             0.26     3.87\n",
      "NOTE:  262        0.01          0.6607     0.2356           0.6705           0.2525     3.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  263        0.01           0.699     0.2663           0.6644            0.245     3.98\n",
      "NOTE:  264        0.01          0.6754     0.2481           0.6626           0.2375     3.74\n",
      "NOTE:  265        0.01          0.6989     0.2611             0.66           0.2475     4.22\n",
      "NOTE:  266        0.01            0.69     0.2534           0.6638            0.245     3.74\n",
      "NOTE:  267        0.01          0.6704     0.2486           0.6578           0.2325     3.96\n",
      "NOTE:  268        0.01          0.6562     0.2303           0.6673            0.255     3.99\n",
      "NOTE:  269        0.01          0.6663     0.2481           0.6625            0.245     4.25\n",
      "NOTE:  270        0.01          0.6568     0.2365           0.6577           0.2425     3.91\n",
      "NOTE:  271        0.01          0.6647      0.249           0.6521           0.2425     3.78\n",
      "NOTE:  272        0.01          0.6779     0.2476           0.6536             0.25     4.23\n",
      "NOTE:  273        0.01          0.6691     0.2481           0.6484             0.24     4.15\n",
      "NOTE:  274        0.01          0.6785     0.2442           0.6562           0.2225     3.83\n",
      "NOTE:  275        0.01          0.6809     0.2558           0.6508             0.23     4.24\n",
      "NOTE:  276        0.01          0.6665     0.2471           0.6489           0.2375     4.01\n",
      "NOTE:  277        0.01          0.6532     0.2409           0.6426           0.2325     4.00\n",
      "NOTE:  278        0.01          0.6198      0.225           0.6663           0.2525     4.08\n",
      "NOTE:  279        0.01          0.6331     0.2313           0.6602             0.24     4.12\n",
      "NOTE:  280        0.01          0.6674     0.2447           0.6506             0.24     4.07\n",
      "NOTE:  281        0.01          0.6485     0.2356           0.6389            0.235     4.23\n",
      "NOTE:  282        0.01          0.6658     0.2476           0.6461            0.235     3.74\n",
      "NOTE:  283        0.01          0.6398     0.2313           0.6434           0.2275     4.00\n",
      "NOTE:  284        0.01          0.6304     0.2365           0.6374            0.235     3.81\n",
      "NOTE:  285        0.01          0.6522     0.2418           0.6343             0.22     4.03\n",
      "NOTE:  286        0.01          0.7056     0.2663           0.6368             0.24     4.12\n",
      "NOTE:  287        0.01          0.6145     0.2125           0.6347           0.2325     3.95\n",
      "NOTE:  288        0.01          0.6243      0.224           0.6454           0.2275     4.13\n",
      "NOTE:  289        0.01          0.6153      0.225           0.6343           0.2325     3.94\n",
      "NOTE:  290        0.01          0.6332     0.2341           0.6391             0.23     4.20\n",
      "NOTE:  291        0.01          0.6364     0.2365           0.6349             0.22     3.76\n",
      "NOTE:  292        0.01          0.6243      0.224           0.6328            0.215     3.93\n",
      "NOTE:  293        0.01            0.62     0.2337           0.6307            0.235     4.15\n",
      "NOTE:  294        0.01          0.6246     0.2274           0.6321           0.2075     3.97\n",
      "NOTE:  295        0.01          0.6356     0.2298           0.6282           0.2175     3.93\n",
      "NOTE:  296        0.01          0.6101     0.2288            0.635             0.22     3.91\n",
      "NOTE:  297        0.01          0.6552     0.2337           0.6272             0.22     3.98\n",
      "NOTE:  298        0.01          0.6657     0.2476           0.6221           0.2125     3.85\n",
      "NOTE:  299        0.01          0.6083     0.2298           0.6314             0.23     4.06\n",
      "NOTE:  300        0.01          0.6193     0.2255           0.6251           0.2275     4.15\n",
      "NOTE:  301        0.01          0.6122      0.225           0.6258            0.215     4.00\n",
      "NOTE:  302        0.01          0.6116      0.224           0.6246           0.2225     4.10\n",
      "NOTE:  303        0.01          0.6345     0.2236           0.6144           0.2125     3.92\n",
      "NOTE:  304        0.01          0.6392     0.2375           0.6313           0.2225     4.03\n",
      "NOTE:  305        0.01          0.6389     0.2216           0.6132           0.2075     4.21\n",
      "NOTE:  306        0.01          0.6457     0.2375           0.6094           0.2075     3.94\n",
      "NOTE:  307        0.01          0.6136     0.2207           0.6138           0.2025     4.24\n",
      "NOTE:  308        0.01          0.6336     0.2317           0.6182           0.2125     4.14\n",
      "NOTE:  309        0.01          0.5984     0.2226           0.6129           0.2075     3.84\n",
      "NOTE:  310        0.01          0.6034      0.225           0.6162            0.225     4.24\n",
      "NOTE:  311        0.01          0.6039     0.2231           0.6142           0.2075     3.73\n",
      "NOTE:  312        0.01          0.6174     0.2303           0.6081            0.215     3.95\n",
      "NOTE:  313        0.01          0.6296     0.2423            0.611           0.2075     3.98\n",
      "NOTE:  314        0.01          0.5799     0.2163           0.6044           0.2025     3.88\n",
      "NOTE:  315        0.01          0.6348     0.2389           0.6055            0.215     3.71\n",
      "NOTE:  316        0.01          0.6153     0.2356           0.6015           0.2125     4.15\n",
      "NOTE:  317        0.01          0.5714     0.2038           0.6134            0.215     4.29\n",
      "NOTE:  318        0.01          0.5898     0.2139           0.6058            0.205     4.17\n",
      "NOTE:  319        0.01          0.6215     0.2298           0.6226             0.22     3.89\n",
      "NOTE:  320        0.01          0.5988     0.2101           0.6187            0.205     4.10\n",
      "NOTE:  321        0.01          0.6149     0.2346           0.5923           0.2025     4.07\n",
      "NOTE:  322        0.01          0.6114      0.225           0.5945              0.2     4.24\n",
      "NOTE:  323        0.01          0.6178     0.2337           0.6027           0.2075     3.86\n",
      "NOTE:  324        0.01          0.6411      0.224            0.599           0.2075     4.21\n",
      "NOTE:  325        0.01          0.5836     0.2144           0.5977           0.2125     4.21\n",
      "NOTE:  326        0.01           0.587     0.2231           0.6114             0.21     4.04\n",
      "NOTE:  327        0.01          0.5647     0.2053           0.6086             0.21     4.00\n",
      "NOTE:  328        0.01          0.5801     0.2139           0.5949           0.1975     3.98\n",
      "NOTE:  329        0.01          0.6046     0.2231           0.5883           0.1975     3.93\n",
      "NOTE:  330        0.01          0.5839     0.2168           0.5911            0.205     3.96\n",
      "NOTE:  331        0.01          0.6195      0.224           0.5899             0.21     3.85\n",
      "NOTE:  332        0.01          0.5871     0.2322           0.6034             0.21     3.98\n",
      "NOTE:  333        0.01          0.6396     0.2433             0.59            0.205     4.13\n",
      "NOTE:  334        0.01          0.6231     0.2298           0.6043           0.2025     4.15\n",
      "NOTE:  335        0.01          0.6288     0.2399           0.5831           0.1975     3.88\n",
      "NOTE:  336        0.01          0.5971     0.2087           0.5811            0.195     3.71\n",
      "NOTE:  337        0.01          0.6135     0.2351           0.6031           0.2125     4.11\n",
      "NOTE:  338        0.01          0.5991     0.2212           0.5827           0.2025     3.98\n",
      "NOTE:  339        0.01          0.5931     0.2106           0.5813           0.2025     4.35\n",
      "NOTE:  340        0.01          0.5571     0.2091           0.5948             0.22     3.89\n",
      "NOTE:  341        0.01          0.5843     0.2087           0.5918             0.21     4.13\n",
      "NOTE:  342        0.01          0.5833     0.2144           0.5804           0.1975     3.95\n",
      "NOTE:  343        0.01          0.6087     0.2303           0.5926              0.2     4.14\n",
      "NOTE:  344        0.01          0.5988     0.2274           0.5774              0.2     4.19\n",
      "NOTE:  345        0.01          0.6056     0.2231           0.5781           0.2075     4.15\n",
      "NOTE:  346        0.01          0.5886     0.2255            0.574            0.195     4.23\n",
      "NOTE:  347        0.01          0.6101     0.2346           0.5735            0.195     3.67\n",
      "NOTE:  348        0.01          0.5827     0.2221           0.5841           0.2025     3.88\n",
      "NOTE:  349        0.01          0.5665     0.2024           0.5772           0.2025     4.15\n",
      "NOTE:  350        0.01          0.5806     0.2173           0.5955             0.21     4.14\n",
      "NOTE:  351        0.01          0.5554     0.2014           0.5704            0.195     4.08\n",
      "NOTE:  352        0.01          0.6115     0.2245           0.5689           0.2025     3.98\n",
      "NOTE:  353        0.01           0.594     0.2144            0.573            0.195     3.85\n",
      "NOTE:  354        0.01          0.5749     0.2168           0.5635           0.1975     4.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  355        0.01          0.5706     0.2062           0.5737           0.2025     3.90\n",
      "NOTE:  356        0.01          0.5854     0.2337           0.5709              0.2     4.05\n",
      "NOTE:  357        0.01          0.5804     0.2154           0.5691            0.195     3.85\n",
      "NOTE:  358        0.01           0.578      0.213           0.5639            0.185     3.82\n",
      "NOTE:  359        0.01          0.5624     0.2183           0.5641           0.1975     3.95\n",
      "NOTE:  360        0.01          0.6009     0.2178            0.593             0.21     4.34\n",
      "NOTE:  361        0.01          0.5825      0.225           0.5644           0.1925     4.08\n",
      "NOTE:  362        0.01           0.572     0.2226           0.5577           0.1975     3.92\n",
      "NOTE:  363        0.01          0.5332     0.1976           0.5627              0.2     4.04\n",
      "NOTE:  364        0.01          0.5558     0.1938            0.568           0.1975     4.26\n",
      "NOTE:  365        0.01           0.578     0.2168           0.5906           0.2075     4.08\n",
      "NOTE:  366        0.01          0.5463     0.1942            0.556           0.1975     3.84\n",
      "NOTE:  367        0.01          0.5644     0.2159           0.5592             0.19     3.99\n",
      "NOTE:  368        0.01          0.5519     0.1962           0.5659             0.18     4.01\n",
      "NOTE:  369        0.01          0.5666     0.2048           0.5747           0.1925     4.16\n",
      "NOTE:  370        0.01          0.5536     0.1995           0.5629             0.19     4.12\n",
      "NOTE:  371        0.01          0.5707     0.2192           0.5515           0.1925     3.85\n",
      "NOTE:  372        0.01          0.5474     0.1952           0.5518            0.195     3.99\n",
      "NOTE:  373        0.01          0.5555     0.2034           0.5558            0.185     4.38\n",
      "NOTE:  374        0.01          0.5776     0.2072           0.5552           0.1925     3.96\n",
      "NOTE:  375        0.01          0.5673     0.2101           0.5605           0.1875     4.31\n",
      "NOTE:  376        0.01          0.5368     0.2029           0.5522           0.1925     3.88\n",
      "NOTE:  377        0.01           0.548     0.2082           0.5477             0.18     3.89\n",
      "NOTE:  378        0.01          0.5797     0.2149             0.55            0.195     3.71\n",
      "NOTE:  379        0.01          0.5598     0.2014           0.5613           0.1975     3.75\n",
      "NOTE:  380        0.01           0.571     0.2091           0.5698           0.1925     4.22\n",
      "NOTE:  381        0.01          0.5363     0.1952           0.5543            0.195     3.99\n",
      "NOTE:  382        0.01           0.559     0.2029           0.5453           0.1975     4.21\n",
      "NOTE:  383        0.01          0.5871     0.2236           0.5767              0.2     3.68\n",
      "NOTE:  384        0.01          0.5635     0.2135           0.5387           0.1825     4.28\n",
      "NOTE:  385        0.01          0.5406     0.1957           0.5409            0.185     3.92\n",
      "NOTE:  386        0.01          0.5751     0.2101           0.5353           0.1825     4.07\n",
      "NOTE:  387        0.01          0.5574     0.2067           0.5355           0.1775     4.06\n",
      "NOTE:  388        0.01          0.5573     0.2048           0.5457           0.1925     3.79\n",
      "NOTE:  389        0.01          0.5408     0.2062           0.5364             0.19     3.87\n",
      "NOTE:  390        0.01          0.5383     0.1942           0.5558           0.1825     4.06\n",
      "NOTE:  391        0.01          0.5548     0.2053           0.5785            0.195     4.13\n",
      "NOTE:  392        0.01           0.544     0.2038           0.5584           0.1925     3.83\n",
      "NOTE:  393        0.01          0.5581     0.2096           0.5533            0.185     4.25\n",
      "NOTE:  394        0.01          0.5457        0.2           0.5321            0.185     4.11\n",
      "NOTE:  395        0.01          0.5184      0.201            0.533           0.1725     4.13\n",
      "NOTE:  396        0.01          0.5595        0.2            0.542           0.1725     4.18\n",
      "NOTE:  397        0.01          0.5246        0.2           0.5234           0.1775     4.04\n",
      "NOTE:  398        0.01          0.5387     0.1904           0.5243           0.1775     4.06\n",
      "NOTE:  399        0.01          0.5258     0.1904           0.5403            0.175     3.95\n",
      "NOTE:  400        0.01          0.5255      0.199           0.5343           0.1925     3.96\n",
      "NOTE:  401        0.01          0.5167     0.1928           0.5225           0.1775     3.79\n",
      "NOTE:  402        0.01          0.5043     0.1846           0.5334            0.175     3.83\n",
      "NOTE:  403        0.01          0.5475        0.2            0.646           0.2275     3.96\n",
      "NOTE:  404        0.01          0.5403     0.2082           0.5356            0.185     3.82\n",
      "NOTE:  405        0.01          0.5235     0.2019            0.523             0.18     3.84\n",
      "NOTE:  406        0.01           0.547     0.1995           0.5307            0.185     4.20\n",
      "NOTE:  407        0.01          0.5059      0.188           0.5329            0.175     3.77\n",
      "NOTE:  408        0.01          0.5152     0.1957            0.521           0.1775     4.11\n",
      "NOTE:  409        0.01          0.5427     0.2053           0.5231             0.18     3.68\n",
      "NOTE:  410        0.01          0.5271     0.2101           0.5316           0.1775     3.88\n",
      "NOTE:  411        0.01          0.5481     0.2038           0.5432           0.1725     4.16\n",
      "NOTE:  412        0.01          0.5319     0.1981            0.527            0.175     4.17\n",
      "NOTE:  413        0.01          0.5265     0.2034            0.528            0.185     3.86\n",
      "NOTE:  414        0.01          0.5271     0.1986           0.5507           0.1775     4.30\n",
      "NOTE:  415        0.01          0.5069     0.1938           0.5207           0.1675     4.03\n",
      "NOTE:  416        0.01          0.5005     0.1875           0.5214             0.19     4.23\n",
      "NOTE:  417        0.01          0.5636     0.2091           0.5152            0.175     4.05\n",
      "NOTE:  418        0.01          0.4964     0.1861           0.5301             0.18     4.15\n",
      "NOTE:  419        0.01          0.5113     0.1889           0.5274           0.1725     3.90\n",
      "NOTE:  420        0.01          0.5175      0.201           0.5122           0.1775     4.11\n",
      "NOTE:  421        0.01          0.5089     0.1822           0.5272           0.1725     3.85\n",
      "NOTE:  422        0.01          0.5173     0.1923           0.5201            0.165     3.88\n",
      "NOTE:  423        0.01          0.4942      0.187           0.5338           0.1775     4.01\n",
      "NOTE:  424        0.01          0.5317     0.1986            0.579            0.195     4.13\n",
      "NOTE:  425        0.01          0.5202     0.1923           0.5357            0.175     3.74\n",
      "NOTE:  426        0.01          0.5176     0.1803           0.5573             0.18     4.05\n",
      "NOTE:  427        0.01          0.5276     0.1957           0.5278           0.1775     4.10\n",
      "NOTE:  428        0.01          0.5074     0.1894           0.5313           0.1775     4.09\n",
      "NOTE:  429        0.01          0.5457      0.199           0.5158           0.1675     4.22\n",
      "NOTE:  430        0.01           0.523     0.1938           0.5097             0.17     4.29\n",
      "NOTE:  431        0.01           0.487     0.1692            0.507           0.1675     3.57\n",
      "NOTE:  432        0.01          0.5054     0.1909           0.5091            0.185     3.97\n",
      "NOTE:  433        0.01          0.5062      0.188           0.4992           0.1725     3.91\n",
      "NOTE:  434        0.01          0.5047     0.1755           0.5057             0.17     4.07\n",
      "NOTE:  435        0.01          0.4912     0.1846            0.509           0.1625     4.06\n",
      "NOTE:  436        0.01          0.5261     0.2019           0.5003             0.17     4.05\n",
      "NOTE:  437        0.01           0.509     0.1856           0.5128           0.1725     4.11\n",
      "NOTE:  438        0.01           0.507     0.1894           0.4975           0.1625     3.94\n",
      "NOTE:  439        0.01          0.4736      0.175           0.5047           0.1825     4.00\n",
      "NOTE:  440        0.01          0.4876     0.1798           0.5353             0.19     3.78\n",
      "NOTE:  441        0.01          0.5156     0.1812           0.5256             0.18     4.00\n",
      "NOTE:  442        0.01          0.5181     0.1928           0.5016             0.17     4.02\n",
      "NOTE:  443        0.01          0.5101     0.1856           0.5027           0.1675     3.99\n",
      "NOTE:  444        0.01          0.5249     0.1875            0.504           0.1675     3.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  445        0.01          0.4791     0.1812           0.5067            0.175     3.87\n",
      "NOTE:  446        0.01          0.4617     0.1615           0.5002             0.17     3.83\n",
      "NOTE:  447        0.01          0.4909     0.1837           0.5528            0.185     3.94\n",
      "NOTE:  448        0.01          0.4752     0.1784           0.5012           0.1625     4.00\n",
      "NOTE:  449        0.01          0.4982     0.1894           0.4985           0.1675     3.82\n",
      "NOTE:  450        0.01          0.4616     0.1745           0.4966            0.165     3.79\n",
      "NOTE:  451        0.01          0.5461     0.2082           0.5963              0.2     4.18\n",
      "NOTE:  452        0.01          0.4772     0.1755           0.4945           0.1725     4.31\n",
      "NOTE:  453        0.01           0.466     0.1673           0.4934           0.1675     4.00\n",
      "NOTE:  454        0.01          0.4905     0.1827           0.4931           0.1775     3.89\n",
      "NOTE:  455        0.01          0.4969     0.1841            0.502           0.1625     4.07\n",
      "NOTE:  456        0.01          0.4711     0.1731           0.4894           0.1675     4.07\n",
      "NOTE:  457        0.01          0.4618     0.1692           0.5244            0.175     3.76\n",
      "NOTE:  458        0.01          0.4509     0.1673           0.4949             0.17     3.93\n",
      "NOTE:  459        0.01          0.4881     0.1764            0.496           0.1625     3.77\n",
      "NOTE:  460        0.01          0.4763     0.1769           0.4922           0.1625     3.96\n",
      "NOTE:  461        0.01          0.5115     0.1947           0.4899           0.1625     3.93\n",
      "NOTE:  462        0.01           0.474     0.1716           0.5137             0.16     3.88\n",
      "NOTE:  463        0.01          0.4931     0.1851           0.5075             0.16     4.23\n",
      "NOTE:  464        0.01            0.49     0.1899           0.5291           0.1725     4.00\n",
      "NOTE:  465        0.01          0.4915      0.187            0.484           0.1575     4.05\n",
      "NOTE:  466        0.01          0.5026     0.1837           0.4845           0.1725     3.94\n",
      "NOTE:  467        0.01          0.4469     0.1673           0.4918           0.1625     4.38\n",
      "NOTE:  468        0.01          0.4663      0.175           0.5094           0.1625     4.02\n",
      "NOTE:  469        0.01          0.4857     0.1798           0.4826            0.155     4.02\n",
      "NOTE:  470        0.01           0.447     0.1683           0.5063           0.1675     3.93\n",
      "NOTE:  471        0.01          0.4817     0.1861           0.4782            0.155     3.85\n",
      "NOTE:  472        0.01          0.4633     0.1745           0.5198            0.185     4.04\n",
      "NOTE:  473        0.01          0.5043     0.1846           0.4972           0.1725     3.78\n",
      "NOTE:  474        0.01          0.4799     0.1817            0.489            0.175     3.90\n",
      "NOTE:  475        0.01          0.4757     0.1851           0.4899           0.1625     4.12\n",
      "NOTE:  476        0.01          0.4645      0.176           0.4822            0.165     4.06\n",
      "NOTE:  477        0.01          0.5077     0.1865           0.5412           0.2075     4.05\n",
      "NOTE:  478        0.01          0.4732     0.1688           0.4995           0.1775     4.13\n",
      "NOTE:  479        0.01           0.476     0.1788           0.4877            0.155     4.27\n",
      "NOTE:  480        0.01          0.4561     0.1659           0.4816           0.1625     3.94\n",
      "NOTE:  481        0.01          0.4643     0.1707           0.4797           0.1625     4.12\n",
      "NOTE:  482        0.01          0.4551     0.1688           0.4935             0.16     3.84\n",
      "NOTE:  483        0.01          0.4639     0.1625           0.4998           0.1675     3.83\n",
      "NOTE:  484        0.01          0.5231     0.1865           0.4885           0.1675     4.14\n",
      "NOTE:  485        0.01          0.4552     0.1683            0.477           0.1625     4.12\n",
      "NOTE:  486        0.01          0.4948      0.176           0.5241             0.18     4.32\n",
      "NOTE:  487        0.01          0.4685      0.174           0.4674           0.1575     3.71\n",
      "NOTE:  488        0.01          0.4368     0.1582           0.5095           0.1825     3.87\n",
      "NOTE:  489        0.01          0.4735     0.1812            0.477           0.1625     3.83\n",
      "NOTE:  490        0.01          0.4528     0.1716           0.4885           0.1675     3.73\n",
      "NOTE:  491        0.01          0.4593     0.1659           0.4876             0.17     3.94\n",
      "NOTE:  492        0.01          0.4389     0.1519           0.4721             0.16     3.81\n",
      "NOTE:  493        0.01          0.4426     0.1563           0.4778           0.1625     4.01\n",
      "NOTE:  494        0.01          0.4516     0.1663           0.4657            0.155     4.15\n",
      "NOTE:  495        0.01          0.4679     0.1716           0.4954           0.1625     4.15\n",
      "NOTE:  496        0.01          0.4422     0.1611           0.4678             0.17     3.93\n",
      "NOTE:  497        0.01          0.4569     0.1707           0.4769            0.165     4.06\n",
      "NOTE:  498        0.01          0.4316      0.162           0.4727           0.1625     4.00\n",
      "NOTE:  499        0.01           0.476     0.1707           0.4752           0.1575     4.00\n",
      "NOTE:  500        0.01          0.4746     0.1788           0.4966           0.1725     4.35\n",
      "NOTE:  501        0.01          0.4621     0.1683           0.4899            0.165     3.93\n",
      "NOTE:  502        0.01           0.476     0.1856           0.4765           0.1625     3.92\n",
      "NOTE:  503        0.01          0.4847     0.1769           0.4689            0.165     3.84\n",
      "NOTE:  504        0.01          0.4952     0.1865           0.4688            0.155     3.91\n",
      "NOTE:  505        0.01          0.4768      0.175           0.4713           0.1575     3.86\n",
      "NOTE:  506        0.01          0.4389     0.1678           0.5516           0.1825     3.81\n",
      "NOTE:  507        0.01          0.4532     0.1712           0.4855           0.1725     3.74\n",
      "NOTE:  508        0.01          0.4774     0.1649           0.4566             0.15     4.12\n",
      "NOTE:  509        0.01          0.4623     0.1625           0.4678             0.16     3.87\n",
      "NOTE:  510        0.01          0.4618     0.1639           0.4585            0.155     4.14\n",
      "NOTE:  511        0.01          0.4437     0.1596           0.4635            0.155     3.90\n",
      "NOTE:  512        0.01          0.4499     0.1678           0.4716           0.1475     4.01\n",
      "NOTE:  513        0.01          0.4746     0.1668           0.4632             0.15     4.00\n",
      "NOTE:  514        0.01           0.439     0.1644           0.4905           0.1575     4.00\n",
      "NOTE:  515        0.01          0.4346      0.175           0.4986           0.1775     3.92\n",
      "NOTE:  516        0.01          0.5187     0.1885           0.4601            0.145     4.13\n",
      "NOTE:  517        0.01          0.4671     0.1683           0.4633             0.16     4.16\n",
      "NOTE:  518        0.01           0.455     0.1731           0.4703           0.1575     3.93\n",
      "NOTE:  519        0.01          0.4631     0.1745           0.4584           0.1475     3.60\n",
      "NOTE:  520        0.01          0.4613     0.1721           0.4833           0.1575     4.15\n",
      "NOTE:  521        0.01          0.4488     0.1683           0.4742           0.1575     4.14\n",
      "NOTE:  522        0.01          0.4198     0.1519           0.4544           0.1525     4.07\n",
      "NOTE:  523        0.01            0.47     0.1716           0.4652           0.1525     3.87\n",
      "NOTE:  524        0.01          0.4292     0.1505           0.4639           0.1425     3.77\n",
      "NOTE:  525        0.01          0.4514     0.1683           0.4629            0.155     3.94\n",
      "NOTE:  526        0.01          0.4313     0.1611           0.4557           0.1625     4.01\n",
      "NOTE:  527        0.01          0.4575     0.1707            0.463           0.1425     4.00\n",
      "NOTE:  528        0.01          0.4423     0.1649           0.4647           0.1525     4.08\n",
      "NOTE:  529        0.01          0.4671     0.1769           0.4672             0.17     4.22\n",
      "NOTE:  530        0.01          0.4277     0.1591           0.4592           0.1625     4.16\n",
      "NOTE:  531        0.01          0.3876     0.1452            0.454             0.15     4.00\n",
      "NOTE:  532        0.01          0.4524     0.1577           0.4515            0.145     3.75\n",
      "NOTE:  533        0.01          0.4556     0.1736           0.4663           0.1725     3.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  534        0.01          0.4426     0.1591           0.4498           0.1375     3.86\n",
      "NOTE:  535        0.01          0.4495     0.1543           0.4634           0.1625     3.93\n",
      "NOTE:  536        0.01          0.4308     0.1524           0.4619           0.1625     4.00\n",
      "NOTE:  537        0.01          0.4298     0.1582           0.4571           0.1625     3.96\n",
      "NOTE:  538        0.01           0.418       0.15           0.4557             0.15     4.00\n",
      "NOTE:  539        0.01          0.4642     0.1716           0.4604           0.1525     3.81\n",
      "NOTE:  540        0.01           0.417     0.1471           0.4794           0.1725     3.94\n",
      "NOTE:  541        0.01          0.4321     0.1625           0.5035            0.165     3.88\n",
      "NOTE:  542        0.01          0.4492     0.1625           0.4549           0.1425     4.03\n",
      "NOTE:  543        0.01          0.4367     0.1538            0.447            0.155     3.87\n",
      "NOTE:  544        0.01          0.4194      0.149           0.4657             0.15     4.33\n",
      "NOTE:  545        0.01          0.4543     0.1731           0.5046             0.17     3.93\n",
      "NOTE:  546        0.01          0.4432     0.1611           0.4692            0.135     3.77\n",
      "NOTE:  547        0.01          0.4404      0.162           0.4613           0.1525     3.97\n",
      "NOTE:  548        0.01          0.4615     0.1736            0.447           0.1425     4.26\n",
      "NOTE:  549        0.01          0.4226     0.1495           0.4469           0.1475     3.79\n",
      "NOTE:  550        0.01          0.4451     0.1673           0.4555           0.1525     4.23\n",
      "NOTE:  551        0.01          0.4345     0.1673           0.4644           0.1575     4.07\n",
      "NOTE:  552        0.01          0.4216     0.1587           0.4845           0.1575     4.06\n",
      "NOTE:  553        0.01            0.46     0.1731           0.4493           0.1425     4.01\n",
      "NOTE:  554        0.01          0.4404     0.1572           0.4652           0.1625     3.94\n",
      "NOTE:  555        0.01          0.3894     0.1375           0.4613            0.155     3.58\n",
      "NOTE:  556        0.01          0.4164     0.1514           0.4521           0.1475     3.96\n",
      "NOTE:  557        0.01          0.4329     0.1567           0.4505           0.1475     4.00\n",
      "NOTE:  558        0.01          0.4382     0.1615           0.4566           0.1575     3.85\n",
      "NOTE:  559        0.01          0.4178     0.1606            0.448           0.1525     4.14\n",
      "NOTE:  560        0.01          0.4255     0.1567           0.4403             0.15     3.96\n",
      "NOTE:  561        0.01          0.4386     0.1611           0.4478            0.145     3.82\n",
      "NOTE:  562        0.01          0.4391     0.1567           0.4416           0.1475     3.99\n",
      "NOTE:  563        0.01          0.4492     0.1635           0.4642           0.1625     4.09\n",
      "NOTE:  564        0.01           0.427     0.1529           0.4513            0.145     3.84\n",
      "NOTE:  565        0.01          0.3808     0.1394            0.447             0.14     4.06\n",
      "NOTE:  566        0.01          0.4354     0.1538           0.4658           0.1425     3.82\n",
      "NOTE:  567        0.01          0.4099     0.1442           0.4589            0.145     4.30\n",
      "NOTE:  568        0.01          0.4089     0.1413           0.4448            0.145     3.90\n",
      "NOTE:  569        0.01          0.4219     0.1601           0.4404            0.145     3.85\n",
      "NOTE:  570        0.01          0.3782     0.1346           0.4384             0.15     3.86\n",
      "NOTE:  571        0.01          0.3924     0.1332           0.4666            0.165     4.22\n",
      "NOTE:  572        0.01          0.4316     0.1558           0.4576            0.145     4.17\n",
      "NOTE:  573        0.01          0.4171     0.1514           0.4554            0.145     4.01\n",
      "NOTE:  574        0.01          0.4267     0.1572           0.4638            0.165     3.95\n",
      "NOTE:  575        0.01          0.3858     0.1433           0.4385           0.1525     3.99\n",
      "NOTE:  576        0.01          0.4367     0.1596           0.4762             0.16     3.87\n",
      "NOTE:  577        0.01          0.3896     0.1447            0.463           0.1525     3.74\n",
      "NOTE:  578        0.01          0.3688     0.1361           0.4621            0.155     3.85\n",
      "NOTE:  579        0.01          0.3933     0.1385           0.4516           0.1475     4.05\n",
      "NOTE:  580        0.01          0.4074     0.1495           0.4448            0.145     3.76\n",
      "NOTE:  581        0.01           0.401     0.1481           0.4609             0.15     3.79\n",
      "NOTE:  582        0.01          0.3968     0.1404           0.4542           0.1575     3.86\n",
      "NOTE:  583        0.01          0.4006      0.149           0.4871             0.17     4.03\n",
      "NOTE:  584        0.01          0.3844     0.1389           0.4444           0.1475     4.04\n",
      "NOTE:  585        0.01          0.4262     0.1567           0.4359           0.1375     3.97\n",
      "NOTE:  586        0.01          0.4027     0.1437           0.4698           0.1425     4.08\n",
      "NOTE:  587        0.01          0.3885     0.1404           0.4315             0.13     3.90\n",
      "NOTE:  588        0.01          0.3953     0.1428           0.4351            0.145     3.69\n",
      "NOTE:  589        0.01          0.3885     0.1404           0.4859            0.165     3.92\n",
      "NOTE:  590        0.01          0.3895     0.1462           0.4453           0.1575     3.84\n",
      "NOTE:  591        0.01          0.4041     0.1481           0.4623            0.145     3.91\n",
      "NOTE:  592        0.01          0.3985     0.1394           0.4384             0.14     3.83\n",
      "NOTE:  593        0.01          0.4014     0.1476           0.4477           0.1425     3.98\n",
      "NOTE:  594        0.01           0.413       0.15           0.4692             0.16     4.02\n",
      "NOTE:  595        0.01          0.3714     0.1351           0.4338            0.135     3.82\n",
      "NOTE:  596        0.01           0.383     0.1365           0.4401            0.145     4.14\n",
      "NOTE:  597        0.01          0.3936     0.1457            0.438             0.16     3.86\n",
      "NOTE:  598        0.01          0.4071     0.1481           0.4422           0.1475     3.92\n",
      "NOTE:  599        0.01          0.3822     0.1385           0.4436            0.155     4.02\n",
      "NOTE:  600        0.01           0.397     0.1543           0.4371             0.14     4.07\n",
      "NOTE:  601        0.01          0.4024     0.1524           0.4293           0.1375     3.88\n",
      "NOTE:  602        0.01          0.4154     0.1481           0.4429           0.1425     3.66\n",
      "NOTE:  603        0.01          0.3991     0.1457           0.4385             0.15     3.81\n",
      "NOTE:  604        0.01           0.392     0.1418           0.4568            0.145     4.05\n",
      "NOTE:  605        0.01          0.3866     0.1341            0.477             0.16     3.79\n",
      "NOTE:  606        0.01          0.3776     0.1327           0.4292           0.1375     3.52\n",
      "NOTE:  607        0.01          0.4031     0.1524           0.4422             0.14     3.94\n",
      "NOTE:  608        0.01          0.3583     0.1313           0.4358           0.1375     4.00\n",
      "NOTE:  609        0.01          0.4286      0.151           0.4538           0.1575     4.13\n",
      "NOTE:  610        0.01          0.4254      0.149           0.4423             0.14     4.04\n",
      "NOTE:  611        0.01          0.4189       0.15           0.4766            0.165     4.00\n",
      "NOTE:  612        0.01          0.3899     0.1394           0.4481            0.155     4.21\n",
      "NOTE:  613        0.01            0.39     0.1462           0.4427           0.1375     3.69\n",
      "NOTE:  614        0.01           0.379     0.1337           0.4547            0.155     4.20\n",
      "NOTE:  615        0.01          0.3954     0.1476           0.4329            0.145     4.06\n",
      "NOTE:  616        0.01          0.4078     0.1466           0.4563           0.1475     4.03\n",
      "NOTE:  617        0.01          0.4181     0.1587           0.4579           0.1575     4.20\n",
      "NOTE:  618        0.01          0.4062     0.1524           0.4444           0.1575     3.96\n",
      "NOTE:  619        0.01          0.3799     0.1365           0.4384            0.125     4.03\n",
      "NOTE:  620        0.01          0.3823     0.1428           0.4316             0.14     3.99\n",
      "NOTE:  621        0.01          0.3819     0.1423           0.4303           0.1375     3.96\n",
      "NOTE:  622        0.01            0.39     0.1462           0.4385           0.1575     3.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  623        0.01          0.4181     0.1563            0.519           0.1675     3.76\n",
      "NOTE:  624        0.01          0.3983     0.1466           0.4343             0.15     4.13\n",
      "NOTE:  625        0.01          0.3798     0.1389           0.4373            0.135     4.20\n",
      "NOTE:  626        0.01          0.3851     0.1399           0.4298           0.1375     3.72\n",
      "NOTE:  627        0.01          0.3615     0.1303           0.4228           0.1375     3.99\n",
      "NOTE:  628        0.01           0.362     0.1288           0.4328             0.15     4.09\n",
      "NOTE:  629        0.01          0.3684     0.1284           0.4242           0.1325     4.18\n",
      "NOTE:  630        0.01            0.36     0.1284           0.4309           0.1375     4.37\n",
      "NOTE:  631        0.01          0.3903     0.1409           0.5005           0.1625     3.78\n",
      "NOTE:  632        0.01          0.3845     0.1409           0.4292             0.13     4.16\n",
      "NOTE:  633        0.01          0.3753     0.1317           0.4497             0.14     3.73\n",
      "NOTE:  634        0.01          0.3637     0.1341           0.4341            0.135     3.87\n",
      "NOTE:  635        0.01          0.3967      0.149            0.435             0.16     3.86\n",
      "NOTE:  636        0.01          0.3642     0.1284           0.4383            0.145     4.12\n",
      "NOTE:  637        0.01          0.3878     0.1418           0.4644             0.16     3.87\n",
      "NOTE:  638        0.01          0.3596     0.1279           0.4375           0.1375     4.00\n",
      "NOTE:  639        0.01          0.3846     0.1433           0.4744            0.155     4.15\n",
      "NOTE:  640        0.01          0.3852     0.1418           0.4789           0.1525     4.09\n",
      "NOTE:  641        0.01          0.3674      0.137           0.4307           0.1375     3.88\n",
      "NOTE:  642        0.01           0.401     0.1495           0.4577           0.1525     4.02\n",
      "NOTE:  643        0.01          0.3487     0.1322            0.431           0.1275     4.02\n",
      "NOTE:  644        0.01          0.4053     0.1505           0.4382           0.1375     3.75\n",
      "NOTE:  645        0.01          0.3673     0.1346           0.4862             0.17     4.13\n",
      "NOTE:  646        0.01          0.3938     0.1457           0.4517           0.1475     4.07\n",
      "NOTE:  647        0.01          0.3781     0.1385           0.4308             0.13     4.02\n",
      "NOTE:  648        0.01          0.3616     0.1284           0.4454           0.1425     3.82\n",
      "NOTE:  649        0.01          0.3492      0.126           0.4229             0.13     3.92\n",
      "NOTE:  650        0.01          0.4137     0.1505           0.5117           0.1625     4.01\n",
      "NOTE:  651        0.01          0.3454     0.1308           0.4359            0.145     3.98\n",
      "NOTE:  652        0.01          0.3819     0.1409           0.4348           0.1475     3.90\n",
      "NOTE:  653        0.01          0.3408     0.1274           0.4352           0.1475     4.02\n",
      "NOTE:  654        0.01          0.3742     0.1337            0.441             0.15     4.26\n",
      "NOTE:  655        0.01          0.3872     0.1476            0.454           0.1425     4.15\n",
      "NOTE:  656        0.01          0.3781     0.1404           0.4706             0.15     3.96\n",
      "NOTE:  657        0.01            0.35     0.1236            0.422           0.1375     4.07\n",
      "NOTE:  658        0.01          0.3898     0.1409           0.4272           0.1275     4.22\n",
      "NOTE:  659        0.01          0.3523     0.1226           0.4356             0.14     4.04\n",
      "NOTE:  660        0.01          0.3696     0.1413           0.4534            0.135     4.15\n",
      "NOTE:  661        0.01          0.3701     0.1293           0.4249           0.1425     3.91\n",
      "NOTE:  662        0.01          0.3553     0.1236           0.4218           0.1275     4.10\n",
      "NOTE:  663        0.01          0.3696      0.138           0.4271           0.1275     4.17\n",
      "NOTE:  664        0.01          0.3664     0.1337           0.4407           0.1425     3.70\n",
      "NOTE:  665        0.01          0.3383     0.1197           0.4687            0.155     3.92\n",
      "NOTE:  666        0.01          0.3532     0.1308           0.4442            0.135     4.04\n",
      "NOTE:  667        0.01          0.3678     0.1332           0.4614           0.1425     3.70\n",
      "NOTE:  668        0.01           0.407     0.1519           0.4417             0.14     4.09\n",
      "NOTE:  669        0.01          0.3408     0.1231           0.4487             0.15     3.67\n",
      "NOTE:  670        0.01           0.344     0.1212           0.4324           0.1325     3.82\n",
      "NOTE:  671        0.01          0.3462      0.124           0.4403            0.145     3.85\n",
      "NOTE:  672        0.01          0.3595     0.1298           0.4488             0.14     3.75\n",
      "NOTE:  673        0.01          0.3661     0.1437           0.4226            0.145     4.25\n",
      "NOTE:  674        0.01          0.3557     0.1212           0.4253           0.1325     3.99\n",
      "NOTE:  675        0.01          0.3994     0.1457           0.4279            0.135     4.16\n",
      "NOTE:  676        0.01          0.3342      0.124           0.4382           0.1425     3.88\n",
      "NOTE:  677        0.01          0.3405     0.1236           0.4372           0.1375     4.16\n",
      "NOTE:  678        0.01          0.3286     0.1178           0.4202           0.1325     4.10\n",
      "NOTE:  679        0.01          0.3865     0.1361           0.4377           0.1375     4.09\n",
      "NOTE:  680        0.01          0.3417     0.1202           0.4215             0.15     3.85\n",
      "NOTE:  681        0.01          0.3727     0.1375           0.4336             0.14     4.02\n",
      "NOTE:  682        0.01          0.3522      0.125           0.4165            0.135     4.22\n",
      "NOTE:  683        0.01          0.3674     0.1293           0.4488             0.14     4.06\n",
      "NOTE:  684        0.01          0.3704     0.1346           0.4312           0.1325     4.18\n",
      "NOTE:  685        0.01           0.362     0.1303           0.4346           0.1375     4.13\n",
      "NOTE:  686        0.01          0.3271     0.1115           0.4338           0.1475     3.93\n",
      "NOTE:  687        0.01          0.3269     0.1207           0.4358           0.1275     4.22\n",
      "NOTE:  688        0.01          0.3322      0.126           0.4187            0.135     3.86\n",
      "NOTE:  689        0.01          0.3403     0.1264           0.4337           0.1425     4.16\n",
      "NOTE:  690        0.01          0.3705     0.1269           0.4374            0.135     3.90\n",
      "NOTE:  691        0.01          0.3685     0.1457           0.4349            0.145     3.89\n",
      "NOTE:  692        0.01          0.3486     0.1264           0.4537            0.135     3.77\n",
      "NOTE:  693        0.01           0.327     0.1212           0.4375           0.1325     4.09\n",
      "NOTE:  694        0.01          0.3706     0.1365           0.4384             0.15     3.92\n",
      "NOTE:  695        0.01          0.3664     0.1255           0.4376           0.1475     4.21\n",
      "NOTE:  696        0.01          0.3579     0.1337           0.4524             0.15     3.84\n",
      "NOTE:  697        0.01          0.3266     0.1221            0.443             0.14     4.39\n",
      "NOTE:  698        0.01          0.3483     0.1216            0.437           0.1475     3.96\n",
      "NOTE:  699        0.01           0.334     0.1139            0.437            0.145     3.80\n",
      "NOTE:  700        0.01          0.3142     0.1111           0.4217           0.1325     3.97\n",
      "NOTE:  701        0.01          0.3549     0.1284           0.4656           0.1525     3.67\n",
      "NOTE:  702        0.01          0.3727     0.1462           0.4258           0.1375     4.08\n",
      "NOTE:  703        0.01          0.3272     0.1111           0.4667           0.1375     3.98\n",
      "NOTE:  704        0.01          0.3283     0.1192           0.4292            0.145     4.19\n",
      "NOTE:  705        0.01          0.3303     0.1139           0.4568            0.145     4.04\n",
      "NOTE:  706        0.01          0.3723     0.1351           0.4305             0.14     3.93\n",
      "NOTE:  707        0.01           0.327     0.1187            0.427           0.1425     4.15\n",
      "NOTE:  708        0.01           0.335     0.1168             0.42           0.1425     3.96\n",
      "NOTE:  709        0.01          0.3363     0.1178           0.4268           0.1325     4.18\n",
      "NOTE:  710        0.01          0.3288     0.1187           0.4155            0.135     3.95\n",
      "NOTE:  711        0.01          0.3255     0.1197           0.4149            0.135     3.93\n",
      "NOTE:  712        0.01          0.3171     0.1101           0.4468           0.1425     3.78\n",
      "NOTE:  713        0.01          0.3407     0.1245           0.4404             0.14     3.52\n",
      "NOTE:  714        0.01          0.3443      0.125           0.4345             0.14     4.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  715        0.01          0.3215     0.1149           0.4435           0.1425     3.90\n",
      "NOTE:  716        0.01          0.3444     0.1337           0.4213             0.14     3.94\n",
      "NOTE:  717        0.01          0.3281     0.1168           0.4941            0.155     3.97\n",
      "NOTE:  718        0.01          0.3569     0.1346           0.4254            0.135     4.20\n",
      "NOTE:  719        0.01          0.3126     0.1048           0.4186            0.135     3.97\n",
      "NOTE:  720        0.01          0.3217     0.1163           0.4724            0.135     4.21\n",
      "NOTE:  721        0.01          0.3458      0.124           0.4441           0.1575     4.00\n",
      "NOTE:  722        0.01          0.3372     0.1216           0.4186           0.1325     3.91\n",
      "NOTE:  723        0.01          0.3573     0.1226           0.4258             0.14     4.07\n",
      "NOTE:  724        0.01          0.3687     0.1313           0.4261             0.14     3.96\n",
      "NOTE:  725        0.01          0.3492     0.1327           0.4122           0.1425     3.89\n",
      "NOTE:  726        0.01          0.3784     0.1418           0.4257            0.135     3.93\n",
      "NOTE:  727        0.01          0.3112     0.1077           0.4212            0.135     4.20\n",
      "NOTE:  728        0.01          0.3327     0.1144           0.4203           0.1475     4.07\n",
      "NOTE:  729        0.01           0.331     0.1159           0.4297           0.1375     4.13\n",
      "NOTE:  730        0.01          0.3131      0.112           0.4194             0.13     3.94\n",
      "NOTE:  731        0.01          0.3463      0.124           0.4179             0.13     4.01\n",
      "NOTE:  732        0.01          0.3244     0.1168            0.416           0.1325     3.80\n",
      "NOTE:  733        0.01          0.3247     0.1173           0.4715             0.17     4.22\n",
      "NOTE:  734        0.01          0.3089     0.1159           0.4299            0.145     4.12\n",
      "NOTE:  735        0.01          0.3663     0.1341            0.444           0.1425     3.90\n",
      "NOTE:  736        0.01          0.3178     0.1221           0.4199            0.145     4.18\n",
      "NOTE:  737        0.01          0.3263     0.1183           0.4745           0.1525     4.11\n",
      "NOTE:  738        0.01          0.3306     0.1216           0.4368           0.1375     4.38\n",
      "NOTE:  739        0.01          0.3266     0.1216           0.4234           0.1325     3.76\n",
      "NOTE:  740        0.01          0.3178     0.1144           0.4338            0.135     4.03\n",
      "NOTE:  741        0.01          0.3252     0.1135           0.4118             0.13     3.78\n",
      "NOTE:  742        0.01          0.3304      0.125           0.4389           0.1475     3.72\n",
      "NOTE:  743        0.01          0.3468     0.1322           0.4548           0.1375     3.89\n",
      "NOTE:  744        0.01          0.3369     0.1212           0.4029           0.1375     3.88\n",
      "NOTE:  745        0.01          0.3147     0.1062           0.4477           0.1475     3.78\n",
      "NOTE:  746        0.01          0.3158     0.1202           0.4296             0.14     3.94\n",
      "NOTE:  747        0.01          0.3303     0.1101           0.4126            0.135     3.79\n",
      "NOTE:  748        0.01          0.3138     0.1197           0.4414           0.1425     3.85\n",
      "NOTE:  749        0.01          0.3305     0.1284           0.4152            0.145     3.77\n",
      "NOTE:  750        0.01          0.3264     0.1139           0.4411           0.1425     3.72\n",
      "NOTE:  751        0.01          0.3671     0.1389           0.4395            0.145     3.84\n",
      "NOTE:  752        0.01           0.319     0.1202           0.4591           0.1475     4.16\n",
      "NOTE:  753        0.01          0.3371     0.1178            0.489            0.145     3.79\n",
      "NOTE:  754        0.01          0.3315     0.1154           0.4263             0.13     4.12\n",
      "NOTE:  755        0.01          0.3098     0.1173           0.4128           0.1325     3.67\n",
      "NOTE:  756        0.01           0.333     0.1255           0.4246           0.1375     3.89\n",
      "NOTE:  757        0.01          0.3245     0.1187           0.4337           0.1375     4.03\n",
      "NOTE:  758        0.01          0.3091     0.1139           0.4083           0.1375     4.00\n",
      "NOTE:  759        0.01           0.307     0.1154            0.446           0.1375     3.79\n",
      "NOTE:  760        0.01          0.3064     0.1149           0.4707           0.1675     3.96\n",
      "NOTE:  761        0.01          0.3194      0.112           0.4325             0.14     4.10\n",
      "NOTE:  762        0.01          0.3509     0.1288           0.5107           0.1625     4.08\n",
      "NOTE:  763        0.01          0.3305     0.1216           0.4225            0.145     3.89\n",
      "NOTE:  764        0.01           0.324     0.1101           0.4246             0.14     3.74\n",
      "NOTE:  765        0.01          0.3126     0.1082           0.4309           0.1325     3.85\n",
      "NOTE:  766        0.01           0.304     0.1087           0.4386            0.145     3.88\n",
      "NOTE:  767        0.01           0.355     0.1308           0.4904            0.155     3.78\n",
      "NOTE:  768        0.01          0.3271     0.1226           0.4123            0.135     4.14\n",
      "NOTE:  769        0.01          0.3129     0.1135           0.4048            0.135     3.88\n",
      "NOTE:  770        0.01          0.3085     0.1106           0.4162           0.1375     3.93\n",
      "NOTE:  771        0.01          0.3191     0.1178           0.4378           0.1425     3.94\n",
      "NOTE:  772        0.01          0.3187     0.1226           0.4393           0.1425     4.04\n",
      "NOTE:  773        0.01          0.3206     0.1226           0.4243           0.1325     3.80\n",
      "NOTE:  774        0.01           0.335     0.1236           0.4238           0.1325     3.95\n",
      "NOTE:  775        0.01          0.3262     0.1236            0.428           0.1375     3.71\n",
      "NOTE:  776        0.01          0.3274     0.1269           0.4687            0.155     3.83\n",
      "NOTE:  777        0.01          0.3187      0.112           0.4387           0.1375     3.92\n",
      "NOTE:  778        0.01          0.3085      0.101           0.4286            0.135     3.95\n",
      "NOTE:  779        0.01          0.2826      0.101           0.4196           0.1375     3.97\n",
      "NOTE:  780        0.01          0.3372     0.1279           0.4635           0.1575     3.67\n",
      "NOTE:  781        0.01          0.3273     0.1144           0.4124            0.135     3.80\n",
      "NOTE:  782        0.01          0.3152     0.1187           0.4197           0.1375     3.94\n",
      "NOTE:  783        0.01          0.3369      0.126           0.4347           0.1425     4.00\n",
      "NOTE:  784        0.01          0.2946     0.1072           0.4194             0.13     4.02\n",
      "NOTE:  785        0.01           0.291     0.1048           0.4422             0.15     4.16\n",
      "NOTE:  786        0.01          0.3116     0.1096           0.4202           0.1275     4.12\n",
      "NOTE:  787        0.01          0.2989        0.1           0.4899            0.145     3.94\n",
      "NOTE:  788        0.01           0.283     0.1024           0.4401            0.135     4.04\n",
      "NOTE:  789        0.01           0.305     0.1149           0.4732           0.1425     3.80\n",
      "NOTE:  790        0.01          0.3089     0.1159           0.4401             0.14     3.94\n",
      "NOTE:  791        0.01          0.3017     0.1091           0.4136           0.1375     3.91\n",
      "NOTE:  792        0.01          0.3263     0.1139           0.4179           0.1425     3.82\n",
      "NOTE:  793        0.01          0.2705    0.09327           0.5007             0.16     3.94\n",
      "NOTE:  794        0.01          0.3141     0.1101           0.4241            0.135     3.91\n",
      "NOTE:  795        0.01          0.3007     0.1115           0.4345             0.14     3.92\n",
      "NOTE:  796        0.01          0.3259     0.1279           0.4947            0.165     4.09\n",
      "NOTE:  797        0.01          0.2968     0.1043             0.42           0.1375     3.88\n",
      "NOTE:  798        0.01          0.2964     0.1058           0.4393            0.135     3.89\n",
      "NOTE:  799        0.01          0.2934     0.1072           0.4108           0.1325     3.86\n",
      "NOTE:  800        0.01          0.3217     0.1216           0.4368           0.1375     3.77\n",
      "NOTE:  801        0.01          0.2923     0.1067           0.4379             0.14     4.25\n",
      "NOTE:  802        0.01          0.2953    0.09856           0.4953             0.17     3.87\n",
      "NOTE:  803        0.01          0.2883     0.1024           0.4087           0.1475     4.13\n",
      "NOTE:  804        0.01          0.2867    0.09423            0.436             0.14     3.82\n",
      "NOTE:  805        0.01          0.2702    0.09519           0.4382             0.14     4.05\n",
      "NOTE:  806        0.01          0.3176     0.1163           0.4422           0.1425     3.94\n",
      "NOTE:  807        0.01          0.2945     0.1029           0.4254             0.14     3.79\n",
      "NOTE:  808        0.01           0.295     0.1072            0.423            0.135     4.07\n",
      "NOTE:  809        0.01          0.2987     0.1091           0.4396             0.15     4.27\n",
      "NOTE:  810        0.01          0.3221     0.1144           0.4329           0.1475     3.93\n",
      "NOTE:  811        0.01          0.2751     0.1029           0.4297             0.15     3.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  812        0.01          0.2852    0.09471           0.4553           0.1525     4.00\n",
      "NOTE:  813        0.01          0.2788     0.1038             0.44           0.1425     3.75\n",
      "NOTE:  814        0.01          0.3613     0.1293           0.4221           0.1275     3.96\n",
      "NOTE:  815        0.01          0.2864    0.09856           0.4651             0.15     3.84\n",
      "NOTE:  816        0.01          0.2876      0.112            0.434            0.145     4.00\n",
      "NOTE:  817        0.01           0.305     0.1096           0.4258           0.1425     3.82\n",
      "NOTE:  818        0.01          0.2721    0.09615           0.4287             0.14     3.96\n",
      "NOTE:  819        0.01          0.2722    0.09904           0.4155             0.13     4.06\n",
      "NOTE:  820        0.01          0.2905     0.1154           0.4318           0.1425     3.75\n",
      "NOTE:  821        0.01          0.2981    0.09904            0.425             0.14     4.13\n",
      "NOTE:  822        0.01          0.2823     0.1053           0.4444            0.145     3.95\n",
      "NOTE:  823        0.01            0.32     0.1173            0.476             0.14     3.75\n",
      "NOTE:  824        0.01          0.2799    0.09808           0.4427           0.1425     3.87\n",
      "NOTE:  825        0.01          0.2753    0.09856           0.4466             0.15     4.06\n",
      "NOTE:  826        0.01           0.259    0.09615           0.4295           0.1425     3.74\n",
      "NOTE:  827        0.01          0.2848     0.1111           0.4393            0.145     3.83\n",
      "NOTE:  828        0.01           0.279     0.1038           0.4218             0.13     4.19\n",
      "NOTE:  829        0.01          0.2794    0.09471           0.4517           0.1475     3.87\n",
      "NOTE:  830        0.01          0.2818     0.0976            0.428             0.14     3.98\n",
      "NOTE:  831        0.01          0.3016     0.1067           0.4684             0.15     3.79\n",
      "NOTE:  832        0.01          0.2876     0.1029           0.4633           0.1475     4.20\n",
      "NOTE:  833        0.01          0.2806    0.09567           0.4086            0.135     3.88\n",
      "NOTE:  834        0.01          0.2932     0.1187           0.4126            0.135     4.19\n",
      "NOTE:  835        0.01           0.279    0.09904           0.4261            0.135     3.85\n",
      "NOTE:  836        0.01          0.2705     0.1034           0.4398           0.1425     4.15\n",
      "NOTE:  837        0.01          0.2962     0.1043           0.4259            0.145     3.65\n",
      "NOTE:  838        0.01          0.2932     0.1062           0.4301             0.14     3.82\n",
      "NOTE:  839        0.01          0.2853     0.1053            0.432           0.1375     4.13\n",
      "NOTE:  840        0.01          0.2728    0.09952           0.4342           0.1425     3.85\n",
      "NOTE:  841        0.01          0.2782    0.09808           0.4182           0.1375     3.95\n",
      "NOTE:  842        0.01          0.2978     0.1125           0.4579            0.145     4.11\n",
      "NOTE:  843        0.01          0.2789     0.1005            0.456             0.14     4.00\n",
      "NOTE:  844        0.01          0.2799    0.09663           0.4075            0.135     3.89\n",
      "NOTE:  845        0.01          0.2739     0.1062           0.5017           0.1625     3.97\n",
      "NOTE:  846        0.01          0.2963     0.1125           0.4344            0.135     3.97\n",
      "NOTE:  847        0.01          0.3111     0.1192           0.4194           0.1325     4.23\n",
      "NOTE:  848        0.01          0.2722    0.09087           0.4264           0.1375     4.05\n",
      "NOTE:  849        0.01          0.2891    0.09471            0.433           0.1425     3.93\n",
      "NOTE:  850        0.01          0.3035     0.1154            0.466           0.1425     3.86\n",
      "NOTE:  851        0.01          0.2612    0.09423           0.4284           0.1375     3.89\n",
      "NOTE:  852        0.01          0.2895     0.1135           0.4232           0.1475     3.97\n",
      "NOTE:  853        0.01          0.2617    0.09663            0.434           0.1375     3.96\n",
      "NOTE:  854        0.01          0.2749    0.09856           0.4199           0.1325     3.93\n",
      "NOTE:  855        0.01          0.2587    0.09087           0.4185           0.1325     3.85\n",
      "NOTE:  856        0.01          0.2887     0.1082           0.4648           0.1525     3.92\n",
      "NOTE:  857        0.01           0.261    0.09423           0.4309           0.1475     4.16\n",
      "NOTE:  858        0.01          0.2791     0.1043           0.4449            0.135     4.00\n",
      "NOTE:  859        0.01          0.3225     0.1245           0.4183            0.135     3.91\n",
      "NOTE:  860        0.01          0.2841     0.1087           0.4296             0.14     3.90\n",
      "NOTE:  861        0.01          0.2617    0.08702           0.4213           0.1375     3.95\n",
      "NOTE:  862        0.01          0.2695    0.09375           0.4195           0.1375     3.88\n",
      "NOTE:  863        0.01          0.2748        0.1           0.4845             0.15     4.00\n",
      "NOTE:  864        0.01           0.274    0.09904           0.4202           0.1325     4.12\n",
      "NOTE:  865        0.01          0.2663    0.08942           0.4329             0.14     4.11\n",
      "NOTE:  866        0.01          0.2665     0.1005           0.4638           0.1375     4.16\n",
      "NOTE:  867        0.01          0.2839     0.1043           0.4433           0.1375     3.86\n",
      "NOTE:  868        0.01          0.2615    0.09375           0.4491           0.1325     4.24\n",
      "NOTE:  869        0.01          0.2791     0.1019           0.4407             0.13     3.89\n",
      "NOTE:  870        0.01          0.2656    0.09712           0.4447           0.1425     4.07\n",
      "NOTE:  871        0.01          0.2749    0.09856           0.4328            0.135     3.86\n",
      "NOTE:  872        0.01          0.2621      0.101           0.4809            0.165     4.09\n",
      "NOTE:  873        0.01          0.2638     0.0976           0.4166           0.1275     3.91\n",
      "NOTE:  874        0.01          0.2384     0.0774           0.4332           0.1325     3.84\n",
      "NOTE:  875        0.01          0.3038     0.1082           0.4256           0.1375     4.02\n",
      "NOTE:  876        0.01           0.261     0.0899           0.4366           0.1325     3.89\n",
      "NOTE:  877        0.01          0.2786     0.1062           0.4152            0.125     3.82\n",
      "NOTE:  878        0.01          0.2369    0.07981           0.4268           0.1525     4.22\n",
      "NOTE:  879        0.01          0.2944        0.1           0.4296           0.1325     3.98\n",
      "NOTE:  880        0.01           0.276     0.1014            0.423             0.14     3.54\n",
      "NOTE:  881        0.01           0.275     0.1024           0.4464           0.1275     3.92\n",
      "NOTE:  882        0.01          0.2335    0.08125           0.4182             0.13     3.94\n",
      "NOTE:  883        0.01          0.2787     0.1019           0.4679           0.1475     4.15\n",
      "NOTE:  884        0.01          0.2944     0.1091           0.4727            0.145     3.95\n",
      "NOTE:  885        0.01           0.255    0.08942           0.4347           0.1375     3.75\n",
      "NOTE:  886        0.01          0.2808    0.09856            0.417           0.1375     4.04\n",
      "NOTE:  887        0.01          0.2537    0.09135           0.4979            0.155     3.98\n",
      "NOTE:  888        0.01          0.2761     0.1024           0.4511           0.1425     3.81\n",
      "NOTE:  889        0.01          0.2767     0.1062           0.4397            0.145     3.78\n",
      "NOTE:  890        0.01          0.3309     0.1216           0.4161           0.1325     4.02\n",
      "NOTE:  891        0.01          0.2694    0.09904            0.425           0.1325     3.95\n",
      "NOTE:  892        0.01          0.2641     0.0976           0.4707           0.1575     4.00\n",
      "NOTE:  893        0.01          0.2785    0.09519           0.4346             0.14     3.80\n",
      "NOTE:  894        0.01          0.2598    0.09135           0.4428             0.14     3.98\n",
      "NOTE:  895        0.01          0.2504    0.09808           0.4126           0.1375     4.05\n",
      "NOTE:  896        0.01          0.2525    0.09423           0.4439           0.1375     3.70\n",
      "NOTE:  897        0.01          0.2702    0.09712           0.4195             0.14     3.76\n",
      "NOTE:  898        0.01          0.2624    0.09663           0.4331             0.14     4.18\n",
      "NOTE:  899        0.01          0.2162    0.07404           0.4319             0.13     3.76\n",
      "NOTE:  900        0.01          0.2705    0.09567           0.4363           0.1425     3.85\n",
      "NOTE:  901        0.01          0.3038     0.1144           0.4436             0.14     3.97\n",
      "NOTE:  902        0.01          0.2641     0.1005           0.4466             0.15     4.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  903        0.01          0.2607    0.08558           0.4365             0.14     3.99\n",
      "NOTE:  904        0.01           0.273     0.1019           0.4433             0.14     3.97\n",
      "NOTE:  905        0.01          0.2914     0.1072           0.4182             0.15     3.75\n",
      "NOTE:  906        0.01           0.254    0.09615           0.4518           0.1325     4.14\n",
      "NOTE:  907        0.01          0.2553    0.09567            0.425             0.13     4.28\n",
      "NOTE:  908        0.01          0.2577    0.09712           0.4302           0.1425     4.15\n",
      "NOTE:  909        0.01          0.2613    0.09183           0.4837           0.1475     4.08\n",
      "NOTE:  910        0.01          0.2921     0.1082           0.4487           0.1425     4.01\n",
      "NOTE:  911        0.01          0.2483     0.0875           0.4199           0.1275     4.06\n",
      "NOTE:  912        0.01          0.2479    0.08269           0.4332           0.1425     4.00\n",
      "NOTE:  913        0.01          0.2693    0.09712           0.4358             0.14     4.00\n",
      "NOTE:  914        0.01          0.2941     0.1111           0.4237            0.135     3.99\n",
      "NOTE:  915        0.01          0.2706    0.09375           0.5136           0.1575     4.01\n",
      "NOTE:  916        0.01          0.2695    0.09567           0.4111           0.1325     4.11\n",
      "NOTE:  917        0.01          0.2874     0.1043           0.4956           0.1425     4.08\n",
      "NOTE:  918        0.01          0.2991      0.112           0.4315            0.145     4.14\n",
      "NOTE:  919        0.01          0.2501    0.09038           0.4218           0.1275     4.14\n",
      "NOTE:  920        0.01          0.2207    0.07837           0.4136            0.135     4.14\n",
      "NOTE:  921        0.01          0.2487    0.09038           0.4512             0.15     4.06\n",
      "NOTE:  922        0.01          0.2552    0.09231           0.4103             0.14     3.93\n",
      "NOTE:  923        0.01          0.2569    0.09183           0.4104           0.1325     4.12\n",
      "NOTE:  924        0.01          0.2543    0.09712           0.4328             0.14     3.98\n",
      "NOTE:  925        0.01          0.2403    0.08654           0.4397           0.1375     3.87\n",
      "NOTE:  926        0.01          0.2346    0.08269           0.4512           0.1425     4.07\n",
      "NOTE:  927        0.01          0.2409    0.08702           0.4439            0.145     4.02\n",
      "NOTE:  928        0.01           0.227    0.08125           0.4167           0.1325     4.12\n",
      "NOTE:  929        0.01          0.2534    0.09375           0.4352           0.1425     4.09\n",
      "NOTE:  930        0.01          0.2626        0.1           0.4652             0.15     3.71\n",
      "NOTE:  931        0.01          0.2393    0.09423           0.4279             0.13     3.80\n",
      "NOTE:  932        0.01          0.3147     0.1187           0.4406           0.1425     3.92\n",
      "NOTE:  933        0.01          0.2492    0.09135           0.4147           0.1375     3.75\n",
      "NOTE:  934        0.01            0.23    0.08269           0.4202           0.1425     4.04\n",
      "NOTE:  935        0.01          0.2687    0.09808           0.4285           0.1425     3.94\n",
      "NOTE:  936        0.01           0.248    0.08173           0.4555           0.1425     3.89\n",
      "NOTE:  937        0.01          0.2194    0.08125           0.4277           0.1375     3.91\n",
      "NOTE:  938        0.01          0.2333    0.08846           0.4483           0.1375     3.94\n",
      "NOTE:  939        0.01          0.2369    0.08606           0.4528           0.1475     4.16\n",
      "NOTE:  940        0.01          0.2215    0.08173           0.4216             0.14     4.36\n",
      "NOTE:  941        0.01          0.2571    0.08654           0.4555           0.1325     4.06\n",
      "NOTE:  942        0.01          0.2281    0.07644           0.4701            0.155     4.01\n",
      "NOTE:  943        0.01          0.2724    0.09279           0.4155            0.135     3.92\n",
      "NOTE:  944        0.01          0.2702    0.09904            0.535             0.17     3.94\n",
      "NOTE:  945        0.01           0.314     0.1144            0.438            0.135     3.79\n",
      "NOTE:  946        0.01          0.2637     0.1005           0.4475           0.1375     3.89\n",
      "NOTE:  947        0.01          0.2363    0.08606           0.4235             0.13     3.87\n",
      "NOTE:  948        0.01           0.239    0.08846           0.4077           0.1375     4.12\n",
      "NOTE:  949        0.01          0.2201    0.07837           0.4346           0.1375     3.89\n",
      "NOTE:  950        0.01          0.2571    0.09567           0.4161             0.14     4.14\n",
      "NOTE:  951        0.01          0.2491    0.08606           0.4365            0.145     4.35\n",
      "NOTE:  952        0.01          0.2083    0.07115           0.4543           0.1525     3.89\n",
      "NOTE:  953        0.01          0.2195    0.08029           0.5337             0.17     4.05\n",
      "NOTE:  954        0.01          0.2513    0.09183           0.4271            0.145     4.16\n",
      "NOTE:  955        0.01          0.2215    0.07596           0.4223            0.135     3.83\n",
      "NOTE:  956        0.01          0.2394    0.09279           0.4105           0.1425     4.17\n",
      "NOTE:  957        0.01          0.2385     0.0875           0.4598            0.145     4.29\n",
      "NOTE:  958        0.01          0.2529    0.09615            0.502           0.1625     3.97\n",
      "NOTE:  959        0.01          0.2836     0.1149           0.4237             0.13     3.97\n",
      "NOTE:  960        0.01          0.2558    0.09087           0.5441           0.1775     3.84\n",
      "NOTE:  961        0.01          0.2475    0.09135           0.4456           0.1425     4.09\n",
      "NOTE:  962        0.01            0.21    0.07548           0.4397           0.1375     4.13\n",
      "NOTE:  963        0.01           0.215    0.07596           0.4397            0.145     4.04\n",
      "NOTE:  964        0.01          0.2432    0.08173           0.4291           0.1375     3.78\n",
      "NOTE:  965        0.01            0.22    0.07981           0.5653           0.1825     4.01\n",
      "NOTE:  966        0.01          0.2432    0.09327            0.428           0.1425     3.99\n",
      "NOTE:  967        0.01          0.2555    0.09183           0.4336             0.14     3.77\n",
      "NOTE:  968        0.01          0.2651     0.1005           0.4846             0.15     4.09\n",
      "NOTE:  969        0.01          0.2448    0.09327           0.4393             0.15     3.78\n",
      "NOTE:  970        0.01          0.2759     0.1067           0.4219            0.135     3.86\n",
      "NOTE:  971        0.01          0.2243    0.08558           0.4662           0.1475     4.09\n",
      "NOTE:  972        0.01          0.2136    0.07644           0.4199            0.135     4.15\n",
      "NOTE:  973        0.01          0.2225    0.08413            0.443             0.15     3.95\n",
      "NOTE:  974        0.01          0.2975     0.1106           0.4207            0.135     3.92\n",
      "NOTE:  975        0.01          0.2081     0.0726           0.4506            0.155     3.93\n",
      "NOTE:  976        0.01          0.2201     0.0774           0.4391           0.1425     3.93\n",
      "NOTE:  977        0.01          0.2283    0.07885           0.4244           0.1275     4.10\n",
      "NOTE:  978        0.01           0.245    0.08654            0.436           0.1475     3.75\n",
      "NOTE:  979        0.01          0.2567    0.09519           0.5015           0.1625     3.79\n",
      "NOTE:  980        0.01          0.2414    0.08221            0.415            0.125     3.95\n",
      "NOTE:  981        0.01          0.2728     0.1077           0.4349             0.13     4.04\n",
      "NOTE:  982        0.01          0.2158    0.07019           0.4253             0.13     3.92\n",
      "NOTE:  983        0.01          0.2661    0.09952            0.429             0.14     3.65\n",
      "NOTE:  984        0.01          0.2838     0.1048           0.4502           0.1425     3.97\n",
      "NOTE:  985        0.01          0.2074    0.06731           0.4167           0.1275     3.97\n",
      "NOTE:  986        0.01          0.2461    0.08221           0.4269             0.14     4.39\n",
      "NOTE:  987        0.01          0.2227    0.07644           0.4335            0.125     3.77\n",
      "NOTE:  988        0.01          0.2065     0.0726           0.4217           0.1325     4.25\n",
      "NOTE:  989        0.01          0.2378    0.09135           0.4393           0.1325     3.84\n",
      "NOTE:  990        0.01           0.237    0.08221           0.4448           0.1375     3.86\n",
      "NOTE:  991        0.01          0.2196    0.08606           0.4304           0.1375     3.89\n",
      "NOTE:  992        0.01          0.2311    0.08558           0.4244            0.145     3.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  993        0.01          0.2043    0.07212           0.4256           0.1325     4.00\n",
      "NOTE:  994        0.01          0.2144    0.07692           0.4598           0.1425     3.70\n",
      "NOTE:  995        0.01          0.2039    0.07404           0.4341            0.135     3.79\n",
      "NOTE:  996        0.01          0.2242    0.07356           0.4199            0.145     4.07\n",
      "NOTE:  997        0.01          0.2556    0.09423           0.4087           0.1325     4.13\n",
      "NOTE:  998        0.01          0.2353     0.0899           0.4259             0.14     4.30\n",
      "NOTE:  999        0.01          0.2181    0.07644           0.4483           0.1425     3.74\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is    3983.26 (s).\n",
      "NOTE: Using SEED=16098325 for sampling.\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3196.00 MB.\n",
      "NOTE:  Loading weights cost       0.02 (s).\n",
      "NOTE:  Initializing each layer cost      48.19 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01          0.2919     0.1024           0.2266           0.0775     4.25\n",
      "NOTE:  1          0.01          0.2493     0.0899           0.2284           0.0725     3.87\n",
      "NOTE:  2          0.01          0.2832     0.1034           0.2365           0.0875     3.97\n",
      "NOTE:  3          0.01          0.2709    0.09663           0.2684             0.09     3.90\n",
      "NOTE:  4          0.01          0.2539    0.08942           0.2794           0.0925     3.98\n",
      "NOTE:  5          0.01           0.267    0.09375           0.2457             0.09     3.83\n",
      "NOTE:  6          0.01          0.2456    0.08702           0.2974            0.095     3.79\n",
      "NOTE:  7          0.01           0.216    0.07308           0.2746           0.1025     3.66\n",
      "NOTE:  8          0.01          0.2416    0.08462           0.2784           0.0975     3.99\n",
      "NOTE:  9          0.01          0.2359     0.0851           0.2505           0.0775     3.78\n",
      "NOTE:  10         0.01          0.2546    0.08606           0.2577           0.0775     4.09\n",
      "NOTE:  11         0.01          0.2421    0.09135           0.2609           0.0825     3.78\n",
      "NOTE:  12         0.01           0.262    0.09423           0.2389             0.08     3.76\n",
      "NOTE:  13         0.01          0.2527    0.09135           0.2399            0.085     3.96\n",
      "NOTE:  14         0.01          0.2348    0.08317           0.2681             0.09     3.99\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      58.60 (s).\n",
      "NOTE: Using SEED=1566299970 for sampling.\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3195.00 MB.\n",
      "NOTE:  Loading weights cost       0.02 (s).\n",
      "NOTE:  Initializing each layer cost      34.19 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01          0.2724        0.1           0.1736           0.0575     4.06\n",
      "NOTE:  1          0.01          0.2157    0.07404           0.1692           0.0575     3.94\n",
      "NOTE:  2          0.01          0.2458    0.08317           0.1871             0.06     4.05\n",
      "NOTE:  3          0.01          0.2519    0.08029           0.1952           0.0625     3.91\n",
      "NOTE:  4          0.01          0.2632    0.09808           0.2058           0.0875     3.89\n",
      "NOTE:  5          0.01          0.2446    0.09327           0.2071           0.0725     3.86\n",
      "NOTE:  6          0.01          0.2283    0.08269           0.2047             0.07     4.22\n",
      "NOTE:  7          0.01          0.2287    0.07548           0.2298           0.0925     3.83\n",
      "NOTE:  8          0.01          0.2578        0.1           0.1922           0.0675     4.05\n",
      "NOTE:  9          0.01           0.266     0.1005            0.236              0.1     3.77\n",
      "NOTE:  10         0.01          0.2219    0.08125           0.1756            0.065     3.99\n",
      "NOTE:  11         0.01          0.2659    0.09808           0.1898           0.0625     3.93\n",
      "NOTE:  12         0.01          0.3513     0.1303           0.1846             0.06     3.87\n",
      "NOTE:  13         0.01          0.2385     0.0774           0.2929             0.11     3.73\n",
      "NOTE:  14         0.01          0.2508    0.09327           0.1679            0.055     3.92\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      59.01 (s).\n",
      "NOTE: Using SEED=1854001598 for sampling.\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3196.00 MB.\n",
      "NOTE:  Loading weights cost       0.02 (s).\n",
      "NOTE:  Initializing each layer cost      34.11 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01          0.2189    0.07067           0.2031           0.0625     4.07\n",
      "NOTE:  1          0.01          0.2397     0.0875           0.2475            0.085     4.34\n",
      "NOTE:  2          0.01          0.2126    0.08462           0.2389            0.075     4.00\n",
      "NOTE:  3          0.01          0.2301    0.08365            0.223             0.08     4.32\n",
      "NOTE:  4          0.01          0.2627    0.08942           0.2478           0.0775     4.16\n",
      "NOTE:  5          0.01          0.2248    0.08173           0.2304           0.0775     3.87\n",
      "NOTE:  6          0.01           0.224    0.08798           0.2389           0.0725     3.82\n",
      "NOTE:  7          0.01          0.2415    0.08558           0.2864           0.0925     3.81\n",
      "NOTE:  8          0.01          0.2513    0.08894           0.2323             0.08     4.02\n",
      "NOTE:  9          0.01          0.2501    0.09423           0.2233           0.0725     3.91\n",
      "NOTE:  10         0.01          0.2026    0.07548           0.2286             0.07     4.13\n",
      "NOTE:  11         0.01          0.2446    0.09087           0.3143            0.095     3.70\n",
      "NOTE:  12         0.01          0.2493    0.09279           0.4154           0.1625     4.02\n",
      "NOTE:  13         0.01           0.245    0.09423           0.2564           0.0775     3.95\n",
      "NOTE:  14         0.01          0.2389    0.08654           0.2561            0.085     4.30\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      60.42 (s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Using SEED=474003764 for sampling.\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3195.00 MB.\n",
      "NOTE:  Loading weights cost       0.02 (s).\n",
      "NOTE:  Initializing each layer cost      33.92 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01          0.2584    0.09904           0.2024           0.0675     4.41\n",
      "NOTE:  1          0.01          0.2023    0.07308           0.2267           0.0875     4.46\n",
      "NOTE:  2          0.01           0.271    0.09952            0.255              0.1     3.99\n",
      "NOTE:  3          0.01          0.2418    0.08894           0.2159           0.0725     4.10\n",
      "NOTE:  4          0.01          0.2659     0.1005           0.2654           0.0975     3.73\n",
      "NOTE:  5          0.01            0.23    0.08125           0.2607           0.0925     4.07\n",
      "NOTE:  6          0.01          0.2451     0.0976           0.2218             0.09     4.26\n",
      "NOTE:  7          0.01          0.2339     0.0899           0.2279           0.0725     3.76\n",
      "NOTE:  8          0.01          0.2407     0.0851            0.231             0.08     3.82\n",
      "NOTE:  9          0.01          0.2306    0.07788           0.2914              0.1     3.86\n",
      "NOTE:  10         0.01           0.236    0.07788            0.218           0.0775     3.91\n",
      "NOTE:  11         0.01          0.2624    0.08269           0.2382           0.0875     3.81\n",
      "NOTE:  12         0.01           0.234     0.0774           0.2319           0.0825     4.03\n",
      "NOTE:  13         0.01          0.2156    0.07019           0.2483            0.085     3.87\n",
      "NOTE:  14         0.01          0.2014    0.06923           0.2348           0.0775     3.96\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      60.05 (s).\n",
      "NOTE: Using SEED=84974728 for sampling.\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3196.00 MB.\n",
      "NOTE:  Loading weights cost       0.02 (s).\n",
      "NOTE:  Initializing each layer cost      33.74 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01          0.2195    0.08654           0.1847            0.065     4.40\n",
      "NOTE:  1          0.01          0.2073    0.06875           0.1988            0.065     4.42\n",
      "NOTE:  2          0.01          0.2422    0.08462            0.177           0.0575     3.94\n",
      "NOTE:  3          0.01           0.219     0.0774           0.2188            0.065     4.00\n",
      "NOTE:  4          0.01          0.2014    0.07067           0.2115           0.0725     4.38\n",
      "NOTE:  5          0.01          0.2166    0.07644           0.1787            0.055     3.93\n",
      "NOTE:  6          0.01          0.2428     0.0875           0.1807             0.06     4.03\n",
      "NOTE:  7          0.01          0.2166    0.07644           0.1891            0.065     4.18\n",
      "NOTE:  8          0.01          0.1965    0.07067            0.192             0.06     3.97\n",
      "NOTE:  9          0.01          0.2031    0.07019           0.1953             0.07     4.21\n",
      "NOTE:  10         0.01           0.209    0.07163           0.1928            0.065     3.79\n",
      "NOTE:  11         0.01          0.2305    0.07452           0.2114           0.0825     4.12\n",
      "NOTE:  12         0.01          0.2313    0.08606           0.1893             0.06     4.09\n",
      "NOTE:  13         0.01          0.2059    0.07452           0.1854           0.0675     3.91\n",
      "NOTE:  14         0.01          0.1927    0.06587           0.3124             0.14     4.20\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      61.56 (s).\n",
      "NOTE: Using SEED=1912376840 for sampling.\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3196.00 MB.\n",
      "NOTE:  Loading weights cost       0.01 (s).\n",
      "NOTE:  Initializing each layer cost      34.07 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01          0.2793     0.1038            0.181             0.06     3.95\n",
      "NOTE:  1          0.01          0.2245    0.08462           0.1796            0.055     3.97\n",
      "NOTE:  2          0.01           0.179    0.06683           0.1925             0.07     4.10\n",
      "NOTE:  3          0.01          0.1927    0.05962            0.185            0.065     4.05\n",
      "NOTE:  4          0.01          0.2076    0.06971           0.2137           0.0775     4.00\n",
      "NOTE:  5          0.01          0.1935    0.06587           0.1914           0.0625     4.10\n",
      "NOTE:  6          0.01            0.22    0.07452           0.1872             0.06     4.22\n",
      "NOTE:  7          0.01          0.1956     0.0649           0.1818           0.0625     3.99\n",
      "NOTE:  8          0.01          0.2345    0.09327           0.2025           0.0775     3.81\n",
      "NOTE:  9          0.01          0.1876    0.06106           0.2097            0.075     3.87\n",
      "NOTE:  10         0.01          0.2444    0.08365           0.1819            0.055     4.21\n",
      "NOTE:  11         0.01          0.2341    0.08846           0.1911            0.055     4.26\n",
      "NOTE:  12         0.01          0.2021    0.07019           0.1878           0.0725     3.98\n",
      "NOTE:  13         0.01          0.2278    0.08317            0.188             0.07     4.09\n",
      "NOTE:  14         0.01          0.2078    0.07596           0.1933           0.0725     3.60\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      60.22 (s).\n",
      "NOTE: Using SEED=551679990 for sampling.\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3196.00 MB.\n",
      "NOTE:  Loading weights cost       0.02 (s).\n",
      "NOTE:  Initializing each layer cost      34.07 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01          0.2136    0.07692           0.2332           0.0775     4.30\n",
      "NOTE:  1          0.01           0.232    0.08606           0.1603           0.0475     4.17\n",
      "NOTE:  2          0.01          0.2165    0.07404           0.1611           0.0525     4.25\n",
      "NOTE:  3          0.01          0.2411    0.09231           0.1965            0.075     4.16\n",
      "NOTE:  4          0.01          0.2322    0.08654           0.1596             0.04     3.90\n",
      "NOTE:  5          0.01          0.1916    0.06442           0.1602             0.05     4.12\n",
      "NOTE:  6          0.01          0.2114    0.08029           0.2028            0.065     4.11\n",
      "NOTE:  7          0.01          0.2506     0.0875           0.2118           0.0725     3.77\n",
      "NOTE:  8          0.01          0.2365    0.08462           0.1852            0.065     3.92\n",
      "NOTE:  9          0.01          0.2103    0.07308            0.191           0.0675     4.07\n",
      "NOTE:  10         0.01           0.193    0.07019           0.1723           0.0575     3.94\n",
      "NOTE:  11         0.01           0.227    0.07163           0.1703            0.055     3.96\n",
      "NOTE:  12         0.01          0.2416    0.09375           0.1662             0.05     3.71\n",
      "NOTE:  13         0.01          0.2558    0.09279           0.2324             0.08     4.02\n",
      "NOTE:  14         0.01          0.2228    0.07981           0.1789            0.065     4.09\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      60.47 (s).\n",
      "NOTE: Using SEED=1677305693 for sampling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Training based on existing weights.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3196.00 MB.\n",
      "NOTE:  Loading weights cost       0.02 (s).\n",
      "NOTE:  Initializing each layer cost      34.14 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01          0.2249    0.07308           0.2049             0.08     4.08\n",
      "NOTE:  1          0.01          0.2164    0.07452           0.1807           0.0625     3.87\n",
      "NOTE:  2          0.01          0.2186    0.08558           0.1839           0.0675     4.02\n",
      "NOTE:  3          0.01          0.1804    0.05769           0.2462           0.0975     3.78\n",
      "NOTE:  4          0.01          0.2476    0.08413           0.2307           0.0875     3.80\n",
      "NOTE:  5          0.01          0.2098    0.07404           0.1901           0.0775     3.92\n",
      "NOTE:  6          0.01          0.1948    0.06346           0.1949           0.0725     4.00\n",
      "NOTE:  7          0.01          0.2201    0.07548           0.1928           0.0725     3.91\n",
      "NOTE:  8          0.01          0.2108    0.07596           0.2218           0.0875     3.83\n",
      "NOTE:  9          0.01          0.2202    0.08221           0.2249             0.08     4.01\n",
      "NOTE:  10         0.01          0.2326     0.0875           0.2509              0.1     3.70\n",
      "NOTE:  11         0.01          0.1881    0.06779           0.2085             0.08     3.99\n",
      "NOTE:  12         0.01          0.2122    0.06683           0.1878             0.06     3.93\n",
      "NOTE:  13         0.01          0.2109    0.07019           0.2032           0.0725     4.21\n",
      "NOTE:  14         0.01          0.2512    0.09423           0.1894           0.0625     3.67\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      58.72 (s).\n",
      "NOTE: Using SEED=49456573 for sampling.\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE:  The Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 626437.\n",
      "NOTE:  The approximate memory cost is 3196.00 MB.\n",
      "NOTE:  Loading weights cost       0.01 (s).\n",
      "NOTE:  Initializing each layer cost      34.32 (s).\n",
      "NOTE:  The total number of threads on each worker is 96.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 4.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 384.\n",
      "NOTE:  Target variable: Category\n",
      "NOTE:  Number of levels for the target variable:      5\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: C1.금융            \n",
      "NOTE:  Level      1: C2.IT모바일       \n",
      "NOTE:  Level      2: C3.부동산         \n",
      "NOTE:  Level      3: C4.방송연예      \n",
      "NOTE:  Level      4: C5.중동아프리카\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of text input variables:      1\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error  Validation Loss Validation Error   Time(s)\n",
      "NOTE:  0          0.01          0.1794    0.05673           0.1472           0.0525     4.13\n",
      "NOTE:  1          0.01          0.2074    0.07837           0.1519           0.0525     4.68\n",
      "NOTE:  2          0.01          0.2173    0.07644           0.1561           0.0475     4.25\n",
      "NOTE:  3          0.01          0.2164    0.08269           0.1758           0.0675     4.02\n",
      "NOTE:  4          0.01          0.2095    0.06779           0.1785           0.0725     4.14\n",
      "NOTE:  5          0.01          0.2163    0.07212           0.2148             0.08     4.10\n",
      "NOTE:  6          0.01          0.1806    0.06538           0.1568            0.055     3.95\n",
      "NOTE:  7          0.01          0.2136    0.07596           0.1775           0.0625     4.04\n",
      "NOTE:  8          0.01          0.2307    0.08173           0.1687           0.0525     3.83\n",
      "NOTE:  9          0.01          0.2261    0.08558            0.249           0.1075     4.33\n",
      "NOTE:  10         0.01          0.2204    0.08317           0.1698             0.06     3.90\n",
      "NOTE:  11         0.01          0.2082    0.07115           0.1664             0.06     3.93\n",
      "NOTE:  12         0.01          0.1925    0.06827           0.1739            0.065     4.18\n",
      "NOTE:  13         0.01          0.2014     0.0649           0.1716           0.0625     4.17\n",
      "NOTE:  14         0.01          0.1869    0.06106           0.1566           0.0475     3.75\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is      61.43 (s).\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1000\n",
    "\n",
    "for i in range(10):\n",
    "    res = conn.sampling.stratified(table='article_train', partind=True,\n",
    "                            output=dict(\n",
    "                                casout=dict(name='article_partitioned',\n",
    "                                            replace=True),\n",
    "                                copyvars='ALL',\n",
    "                                partindname='partition'),\n",
    "                            samppct=10)  # sampling rate for validation set\n",
    "                            \n",
    "\n",
    "    train = conn.CASTable(name='article_partitioned', where='partition=0', casout='train')\n",
    "    valid = conn.CASTable(name='article_partitioned', where='partition=1', casout='valid')\n",
    "\n",
    "    model.fit(data=train, inputs='content', texts='content', target='category', nominals='category', valid_table=valid,\n",
    "          text_parms=TextParms(init_input_embeddings='w2v_300'), record_seed=909455678,\n",
    "          mini_batch_size=4, max_epochs=n_epochs, lr=0.01, log_level=2)\n",
    "    \n",
    "    n_epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_to_table('/bigdisk/lax/shpath/korean/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Model weights attached successfully!\n",
      "NOTE: text_classifier_weights_attr.sashdat is used as weigths attribute.\n",
      "NOTE: Model attributes attached successfully!\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('/bigdisk/lax/shpath/korean/text_classifier_weights.sashdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table ARTICLE_TEST in caslib CASUSER(shpath).\n",
      "NOTE: The table ARTICLE_TEST has been created in caslib CASUSER(shpath) from binary data uploaded to Cloud Analytic Services.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table ARTICLE_TEST in caslib CASUSER(shpath).\n",
      "NOTE: The table ARTICLE_TEST has been created in caslib CASUSER(shpath) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; caslib</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASUSER(shpath)</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; tableName</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>ARTICLE_TEST</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; casTable</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>CASTable('ARTICLE_TEST', caslib='CASUSER(shpath)')</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.593s</span> &#183; <span class=\"cas-user\">user 0.365s</span> &#183; <span class=\"cas-sys\">sys 0.208s</span> &#183; <span class=\"cas-memory\">mem 200MB</span></small></p>"
      ],
      "text/plain": [
       "[caslib]\n",
       "\n",
       " 'CASUSER(shpath)'\n",
       "\n",
       "[tableName]\n",
       "\n",
       " 'ARTICLE_TEST'\n",
       "\n",
       "[casTable]\n",
       "\n",
       " CASTable('ARTICLE_TEST', caslib='CASUSER(shpath)')\n",
       "\n",
       "+ Elapsed: 0.593s, user: 0.365s, sys: 0.208s, mem: 200mb"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_test = conn.upload(path + 'news_article_0.sas7bdat', casout=dict(name='article_test', replace=True))\n",
    "test = conn.CASTable(name='article_test', casout='test')\n",
    "test.tail()\n",
    "test_df = pd.DataFrame(conn.CASTable('article_test').to_frame())\n",
    "test_df.rename(columns={'category_answer':'category'}, inplace=True)\n",
    "conn.upload(test_df, casout=dict(name='article_test', replace=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ScoreInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number of Observations Read</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number of Observations Used</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Misclassification Error (%)</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Top 5 Misclassification Error (%)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Loss Error</td>\n",
       "      <td>0.451243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CASUSER(shpath)</td>\n",
       "      <td>Valid_Res_2ulCmk</td>\n",
       "      <td>1000</td>\n",
       "      <td>24</td>\n",
       "      <td>CASTable('Valid_Res_2ulCmk', caslib='CASUSER(s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 38.4s</span> &#183; <span class=\"cas-user\">user 46.7s</span> &#183; <span class=\"cas-sys\">sys 13.2s</span> &#183; <span class=\"cas-memory\">mem 5.77e+03MB</span></small></p>"
      ],
      "text/plain": [
       "[ScoreInfo]\n",
       "\n",
       "                                Descr         Value\n",
       " 0        Number of Observations Read          1000\n",
       " 1        Number of Observations Used          1000\n",
       " 2        Misclassification Error (%)          14.5\n",
       " 3  Top 5 Misclassification Error (%)             0\n",
       " 4                         Loss Error      0.451243\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib              Name  Rows  Columns  \\\n",
       " 0  CASUSER(shpath)  Valid_Res_2ulCmk  1000       24   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Valid_Res_2ulCmk', caslib='CASUSER(s...  \n",
       "\n",
       "+ Elapsed: 38.4s, user: 46.7s, sys: 13.2s, mem: 5.77e+03mb"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.evaluate(data='article_test',top_probs=10, text_parms=TextParms(init_input_embeddings='w2v_300'))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; Crosstab</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"category\">category</th>\n",
       "      <th title=\"C1.금융\">Col1</th>\n",
       "      <th title=\"C2.IT모바일\">Col2</th>\n",
       "      <th title=\"C3.부동산\">Col3</th>\n",
       "      <th title=\"C4.방송연예\">Col4</th>\n",
       "      <th title=\"C5.중동아프리카\">Col5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1.금융</td>\n",
       "      <td>158.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C2.IT모바일</td>\n",
       "      <td>9.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C3.부동산</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C4.방송연예</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C5.중동아프리카</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>186.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.0269s</span> &#183; <span class=\"cas-user\">user 0.0349s</span> &#183; <span class=\"cas-sys\">sys 0.0375s</span> &#183; <span class=\"cas-memory\">mem 6.18MB</span></small></p>"
      ],
      "text/plain": [
       "[Crosstab]\n",
       "\n",
       "     category   Col1   Col2   Col3   Col4   Col5\n",
       " 0      C1.금융  158.0   10.0   18.0    6.0    8.0\n",
       " 1   C2.IT모바일    9.0  178.0    6.0    1.0    6.0\n",
       " 2     C3.부동산   42.0    4.0  148.0    3.0    3.0\n",
       " 3    C4.방송연예    9.0    4.0    1.0  185.0    1.0\n",
       " 4  C5.중동아프리카    3.0    3.0    6.0    2.0  186.0\n",
       "\n",
       "+ Elapsed: 0.0269s, user: 0.0349s, sys: 0.0375s, mem: 6.18mb"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.valid_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
